% reset section counter
%\setcounter{section}{0}

%\metadata{lecture ID}{Your names}{date}
%\metadata{6}{Daniel Do}{February 1st, 2021}
%In this chapter, we will instantiate Rademacher complexity for two important hypothesis classes: linear models and two-layer neural networks. In the process, we will develop margin theory and use it to bound the generalization gap for binary classifiers.
%\tnote{todo}


In this chapter, we will instantiate the Rademacher complexity theory developed in Chapter~\ref{chap:conc} to linear classifiers, and obtain generalization bounds that depend on the margin and norm of the classifier.  In section~\ref{chap:generalization:sec:margin}, we will first present the margin approach for classification problems that are in principle applicable to general hypothesis class. In Section~\ref{lec7:sec:lin_models}, we will bound the Rademacher complexity of linear models and combine it with the margin approach to obtain generalization bounds for linear classifiers.

\sec{Margin-based approach  for classification problems} \label{chap:generalization:sec:margin}

%In order to motivate the definition of margin and its use in analyzing classification problems, we will first clarify some high-level formulation choice for classification problem on what are the hypothesis class and loss functions. 
To motivate the concept of margin and its role in analyzing classification problems, we will first clarify the choice of some key elements in the formulation of classification problems: the hypothesis class and the loss function. 
For example, in~\Cref{example:binary-classficiation-rc}, we considered linear classification, where the model outputs $\sgn(w^\top x \ge 0)$ as the prediction. The most direct approach is to define the family of hypothesis $\cH = \{x\mapsto \sgn(w^\top x \ge 0): w\in \R^d\}$, and the loss function $\ell: \{\pm1\}\times \{\pm 1\}\rightarrow \R$ is defined to be $\ell(\hat{y}, y) =\ind{\hat{y} \neq y}$. One drawback of this formulation is that  the loss function and the model are both discontinuous, which makes them difficult to analyze. More importantly, the hypothesis class is scale-invariant---for any $\alpha> 0$, we have $\sgn(w^\top x \ge 0) = \sgn(\alpha w^\top x \ge 0)$. This means that the formulation does not allow us to consider finer details about the classifier, e.g., the norm of $w$ and the ``confidence'' of the prediction. It's worth noting that $w^\top x$ is the logit in logistic regression, where the model outputs a probability distribution for the two classes. The confidence of the classifier on training examples apriori should somewhat affect the generalization of the classifier, but this formulation does not take it into account.

It turns out that an alternative formulation allows us to take the norm and ``confidence'' of the classifier into account.  We let the hypothesis class be the family of functions that outputs the \textit{real-valued} logits \textit{without the threshold}. For example, for linear classification, we let $\cH = \{x\mapsto w^\top x: w\in \R^d \}$ (or its subsets by restricting the norm of $w$.) We fold the threshold function in the definition of the loss, that is, given a model $h: \cX \rightarrow \R$, let the 0-1 loss for example $(x,y)$ be
\begin{align}
\ellbinary((x,y), h) = \ind{yh(x) \le 0 } \perm \label{eqn:17}
\end{align}
This way, norm constrain on $w$ will be able to affect the complexity of the hypothesis class. However, the loss function is still discontinuous. We will design a surrogate continuous loss function below for analysis.  

%Assume that we are in the same setting as in the previous section. A fundamental problem we face in this setting is that we do not have a continuous loss: everything is discrete in the output space. We need to find a way to reason about the scale of the output. An example of this is logistic regression: the logistic regression model outputs a probability, and when we compare it to the outcome (0 or 1), its closeness to the true output gives us a measure of how confident we are in the prediction.

Now we formalize the notion of margin, which plays an important role in the analysis. 
As shown in Figure \ref{lec6:fig:margin}, intuitively, the black line is a ``better'' or more confident decision boundary than the red line because datapoints are further away from the black boundary than the read boundary.  
We will define the margin of a classifier to be the minimum distance from any point to the decision boundary, and thus the black line has bigger margin than the red line. 
In this section, we will prove that the larger margin and smaller norm will lead to better generalization error. 

\begin{figure}[t]
    \begin{center}
  \includegraphics[width=0.5\textwidth]{figures/margin.png}
  \end{center}
  \caption{The red and black lines are two decision boundaries. The X's are positive examples and the O's are negative examples. The black line has a larger margin than the red line, and is intuitively a better classifier.}
  \label{lec6:fig:margin}
\end{figure}

%\subsec{Formalizing margin theory} \label{sec:formal_margin}
We start with a formal mathematical definition of the margin. We will assume throughout this section that the dataset $\cD = ((x\sp{1}, y\sp{1}), \dots, (x\sp{n}, y\sp{n}))$ is \textit{separable} by the hypothesis class, that is,  exists some $h_\theta\in\cH$ that fits the data with zero error: $\forall i, y^{(i)} = \sgn(h_\theta(x^{(i)}))$. This assumption can be relaxed or weakened, but we assume it to  make the derivation cleaner. The separability assumption is empirically often true because the model family is very expressive and the training error can be close to zero. 

The first definition is the margin of a single example. 
\begin{definition}[(Unnormalized) margin]
The \textit{(unnormalized) margin} of a model $h_\theta$ for example $(x, y)$ is defined as $\margin(x) = yh_\theta(x)$. Typically, margin is only defined on correctedly labeled examples with $\sgn(h_\theta(x)) = y$. Under separability condition, all the datapoints have well-defined margin $\margin(x)\geq 0$. 
\end{definition}

Then, we can define the margin for a training dataset. 
\begin{definition}[Minimum margin] Given a dataset $S = \{(x\sp{1}, y\sp{1}), \dots, (x\sp{n}, y\sp{n})\}$, the \textit{minimum margin} of model $h_\theta$ over the dataset is defined as $\gamma_{\min} \triangleq \min_{i\in [n]} y^{(i)}h_\theta(x^{(i)})$.
\end{definition}

Our final result bound from above the generalization gap by a function of the margin and the parameter norm. In the literature, there are various bounds that involve variants of the definition of the margin. %we could derive based on what margin we use. For this current setting we are using $\gamma_{\min}$, which is the minimum margin, 
For example, some more refined generalization bounds can depend on the margin averaged over the datasets~\citep{srebro2010optimistic,wei2020improved}.

The 0-1 loss in \Cref{eqn:17}  is a discontinuous function of the $\margin(x)$. We introduce a \textit{surrogate loss}, a loss function which approximates zero-one loss but takes the scale of the margin into account. The \textbf{margin loss}, also known as \textbf{ramp loss}, is defined as 
\begin{equation}
    \ell_\gamma(t) = \begin{cases} 
      0, & t\geq \gamma \\
      1, & t\leq 0 \\
      1-t/\gamma, & 0\leq t\leq \gamma
   \end{cases} \label{lec6:eqn:ramp_loss}
\end{equation}

\begin{figure}[ht!]
    \begin{center}
  \includegraphics[width=0.5\textwidth]{figures/margin_loss.png}
  \end{center}
  \caption{Plotted margin loss.\tnotelong{also draw the binary loss; have some label for $x$ axis and $y$ axis}}
  \label{lec6:fig:marginloss}
\end{figure}
Here $\gamma$ is a tunable parameter that will be chosen in the analysis. Figure \ref{lec6:fig:marginloss} plots the ramp loss. For convenience, define $\ell_\gamma((x,y), h) \triangleq \ell_\gamma(yh(x))$. We can view $\ell_\gamma$ as a continuous version of $\err$ that is more sensitive to the scale of the margin on $[0,\gamma]$. Notice that $\err$ is always less than or equal to the $\ell_\gamma$ when $\gamma\geq 0$, i.e.
\begin{equation}
    \err((x,y), h) = \ind{yh(x) < 0}\leq \ell_\gamma(yh(x)) =\ell_\gamma ((x,y), h)
\end{equation}
holds for all $(x,y)\sim P$. Taking the expectation over $(x,y)$ on both sides of this inequality, we see that
\begin{equation}
    L(h) = \Exp_{(x,y)\sim P} \left[ \err((x,y), h) \right] \leq \Exp_{(x,y)\sim P} \left[ \ell_\gamma ((x,y), h) \right].
\end{equation}

Therefore, the population 0-1 loss is bounded by the population margin loss, and so it is sufficient to upperbound the latter, which we will do below. 
Formally, define the population and empirical versions margin losses as:
\begin{equation}
L_\gamma(h) = \Exp_{(x,y)\sim P}\l[ \ell_\gamma((x,y), h)\r], \quad \hat{L}_\gamma(h) = \sum_{i=1}^n\l [\ell_\gamma((x^{(i)},y^{(i)}), h)\r]\perm
\end{equation}
By the machinery of Rademacher complexity (e.g., Corollary \ref{lec6:cor:ggap-rsbound}) we can bound the generalization of the margin loss by the Rademacher complexity of the margin loss: with probability at least $1-\delta$, 
\begin{equation}
L_\gamma(h) - \hat{L}_\gamma(h)\leq 2R_S(\cF) + 3\sqrt{\frac{\log (2/\delta)}{2n}}\,,
\end{equation}
where $\cF = \{(x,y)\mapsto \ell_\gamma((x,y), h)\mid h\in\cH\}$ in the family of margin losses. Note that if we set $\gamma\leq \gamma_{\min}$, then $\forall i, y^{(i)}h(x^{(i)})\geq \gamma_{\min}\ge \gamma$ which implies that $\ell_\gamma((x^{(i)}, y^{(i)}), h)=0$. That is, the training margin loss $\hat{L}_{\gamma}(h) = 0$. 
Therefore, in order to bound $L_\gamma(h)$, it suffices to bound $R_S(\cF)$.
%This follows because by definition of $\gamma_{\min}$, $y^{(i)}h(x^{(i)})\geq \gamma_{\min}$ for any $(x^{(i)}, y^{(i)})\in \cD$. As a result, $\ell_\gamma((x^{(i)}, y^{(i)}), h) = \ell_\gamma(y^{(i)}h(x^{(i)})) = 0$ holds. 

It's only worthwhile going these steps via the margin loss if the Rademacher complexity of the margin loss family $\cF$ can be more amenable to theoretical analysis, which we wil perform next.  We first introduce a tool, called \textit{Talagrand's lemma} or \textit{Lipschitz contraction lemma} that can be used to deal with any Lipschitz loss function or generally Lipschitz composition. We will use to bound $R_S(\cF)$ in terms of $R_S(\cH)$ to remove any dependence on the loss function. 
 
\begin{lemma}[Talagrand's contraction lemma] \label{lec6:lem:talagrand_lemma}
Let $\phi:\R\to\R$ be a $\kappa$-Lipschitz function. Then \begin{equation}
    R_S(\phi\circ \cH)\leq \kappa R_S(\cH),
\end{equation} 
where $\phi\circ\cH = \{z\mapsto \phi(h(z))\mid h\in\cH\}$.
\end{lemma}

We apply the Talagrand's lemma with $\phi(t) = \ell_\gamma(t)$, which is $\frac{1}{\gamma}$-Lipschitz. We can express $\cF$ as $\cF=\ell_\gamma\circ\cH'$ where $\cH' = \{(x,y)\to yh(x)\mid h\in\cH\}$ is the family of the margin (which is a slightly variant of the model family due to the multiplication with the $y$). 
\begin{align}
R_S(\cF) &\leq \frac{1}{\gamma}R_S(\cH') \\
&= \frac{1}{\gamma}\Exp_{\sigma_1,\dots, \sigma_n} \l[ \sup_{h\in \cH} \frac{1}{n} \sum^n_{i=1} \sigma_i y^{(i)}h(x^{(i)}) \r] \\
&= \frac{1}{\gamma}\Exp_{\sigma_1,\dots, \sigma_n} \l[ \sup_{h\in \cH} \frac{1}{n} \sum^n_{i=1} \sigma_i h(x^{(i)})  \r] && \text{bc. $y^{(i)}\sigma_i$ has the same distribution as $\sigma_i$ }\\
&= \frac{1}{\gamma}R_S(\cH).
\end{align}

Putting this all together, we have shown that for $\gamma = \gamma_{\min}$,
\begin{align}
\Err(h) \leq L_\gamma(h) &\leq 0 + O \left( \frac{R_S(\cH)}{\gamma} \right) + O \left( \sqrt{\frac{\log (2 / \delta)}{2n}} \right) \\
&= O \left( \frac{R_S(\cH)}{\min_i y\sp{i} h(x\sp{i}) } \right) + \underbrace{O \left( \sqrt{\frac{\log (2 / \delta)}{2n}} \right)}_{\textup{low order term}}.
\end{align}

%In other words, for training data of the form $S = \{(x\sp{i},y\sp{i})\}_{i=1}^n \subset \mathbb{R}^d \times \{-1,1\}$, a hypothesis class~$\mathcal{H}$ and 0-1 loss, we can derive a bound of the form
%\begin{equation}\label{lec7:eqn:generalization_loss}
%    \text{g} \leq O\left(\frac{R_S(\mathcal{H})}{\gamma_{\mathrm{min}}}\right) + \text{low-order term},
%\end{equation}
%where $\gamma_\mathrm{min}$ is the minimum margin achievable on~$S$ over those hypotheses in $\cH$ that separate the data, and $R_S(\cH)$ is the empirical Rademacher complexity of $\cH$. 
%Such bounds state that simpler models will generalize better beyond the training data, particularly for data that is strongly separable.

%\begin{remark} \label{lec7:rmk:union_bound_margin}
%Consequently, the $\gamma$ we choose to define the hypothesis class is random, which is not a valid choice when thinking about Rademacher complexity! 
	
\noindent{\bf Caveat and Fixes.}
We note that the argument above has a caveat and is not entirely correct. 
In the context of standard Rademacher complexity machinery (e.g., Corollary \ref{lec6:cor:ggap-rsbound}), the dataset $S$ is a  random variable, whereas the loss function $\ell$ and hypothesis class $\cH$ (and the class of losses $\cF$) are  fixed object that cannot depend on the dataset $S$. The technical reason for this requirement is that we need the $\ell_\gamma((x\sp{i}, y\sp{i}), h)$'s to be independent with each other to apply concentration inequalities. This is not true when $\gamma$ depends on the dataset $S$. Therefore, choosing the margin loss $\ell_\gamma$ with $\gamma$ depending on the dataset is not allowed when we invoke Corollary~\ref{lec6:cor:ggap-rsbound}. 

%Technically we cannot apply Talagrand's lemma with a random $\kappa$ (which we took to be $1/\gamma$). 
%Also, when we use concentration inequalities, we implicitly assume that the $\ell_\gamma((x\sp{i}, y\sp{i}), h)$ are independent of each other. That is not the case if $\gamma$ is dependent on the data.

Fortunately, there is a simple fix for this caveat, and in general any similar issues involving a single variable or a constant number of variables could be addressed by an additional layer of union bound as sketched below. 
%We sketch out how one might address this issue below. 
The main idea is to do another union bound over the choice of $\gamma$ so that we have uniform convergence of the set of $\gamma$.  
We choose a grid of $\gamma$ in the log space, that is, define  $\Gamma = \left\{ 2^k: k \in [-B, B] \right\}$ for some sufficiently large $B$ that contains $2B+1$ choices of $\gamma$'s. 
For every fixed $\gamma \in \Gamma$, with probability greater than $1 - \delta/(2B+1)$, we have
\begin{align}
\Err(h) \leq \hatL_\gamma (h) + O \left( \frac{R_S(\cH)}{\gamma} \right) + O \left( \sqrt{\frac{\log \frac{2B+1}{\delta}}{n}} \right).
\end{align}
The, we taking a union bound over all $\gamma \in \Gamma$. Then, with probability greater than $1 - \delta$, for all $\gamma \in \Gamma$, 
\begin{align}
    \Err(h) \leq \hatL_\gamma (h) + O \left( \frac{R_S(\cH)}{\gamma} \right) + O \left( \sqrt{\frac{\log \frac{2B+1}{\delta}}{n}}\right). \label{lec7:eqn:unionboundmargin}
\end{align}
\tnote{sopped editing here}
Last, choose the largest $\gamma \in \Gamma$ such that $\gamma \leq \gamma_{\min}$. Then, for this value of $\gamma$, our desired bound directly follows from the bound in \eqref{lec7:eqn:unionboundmargin}. Namely, we have that $\hatL_{\gamma} (h) = 0$ and $O \left( \frac{R_S(\cH)}{\gamma} \right) = O \left( \frac{R_S(\cH)}{\gamma_{\min}} \right)$. The additional term, $\tilO\left ( \sqrt{\frac{\log B}{n} }\right )$, is the price exacted by the uniform convergence argument required to correct the heuristic bound given in \eqref{lec7:eqn:generalization_loss}.

%\end{remark}
