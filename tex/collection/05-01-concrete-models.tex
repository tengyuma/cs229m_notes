% reset section counter
%\setcounter{section}{0}

%\metadata{lecture ID}{Your names}{date}
%\metadata{6}{Daniel Do}{February 1st, 2021}
%In this chapter, we will instantiate Rademacher complexity for two important hypothesis classes: linear models and two-layer neural networks. In the process, we will develop margin theory and use it to bound the generalization gap for binary classifiers.
%\tnote{todo}


In this chapter, we will instantiate the Rademacher complexity theory developed in Chapter~\ref{chap:conc} to linear classifiers, and obtain generalization bounds that depend on the margin and norm of the classifier.  In section~\ref{chap:generalization:sec:margin}, we will first present the margin approach for classification problems that are in principle applicable to general hypothesis class. In Section~\ref{lec7:sec:lin_models}, we will bound the Rademacher complexity of linear models and combine it with the margin approach to obtain generalization bounds for linear classifiers.

\sec{Margin-based approach  for classification problems} \label{chap:generalization:sec:margin}

%In order to motivate the definition of margin and its use in analyzing classification problems, we will first clarify some high-level formulation choice for classification problem on what are the hypothesis class and loss functions. 
To motivate the concept of margin and its role in analyzing classification problems, we will first clarify the choice of some key elements in the formulation of classification problems: the hypothesis class and the loss function. 
For example, in~\Cref{example:binary-classficiation-rc}, we considered linear classification, where the model outputs $\sgn(w^\top x \ge 0)$ as the prediction. The most direct approach is to define the family of hypothesis $\cH = \{x\mapsto \sgn(w^\top x \ge 0): w\in \R^d\}$, and the loss function $\ell: \{\pm1\}\times \{\pm 1\}\rightarrow \R$ is defined to be $\ell(\hat{y}, y) =\ind{\hat{y} \neq y}$. One drawback of this formulation is that  the loss function and the model are both discontinuous, which makes them difficult to analyze. More importantly, the hypothesis class is scale-invariant---for any $\alpha> 0$, we have $\sgn(w^\top x \ge 0) = \sgn(\alpha w^\top x \ge 0)$. This means that the formulation does not allow us to consider finer details about the classifier, e.g., the norm of $w$ and the ``confidence'' of the prediction. It's worth noting that $w^\top x$ is the logit in logistic regression, where the model outputs a probability distribution for the two classes. The confidence of the classifier on training examples apriori should somewhat affect the generalization of the classifier, but this formulation does not take it into account.

It turns out that an alternative formulation allows us to take the norm and ``confidence'' of the classifier into account.  We let the hypothesis class be the family of functions that outputs the \textit{real-valued} logits \textit{without the threshold}. For example, for linear classification, we let $\cH = \{x\mapsto w^\top x: w\in \R^d \}$ (or its subsets by restricting the norm of $w$.) We fold the threshold function in the definition of the loss, that is, given a model $h: \cX \rightarrow \R$, let the 0-1 loss for example $(x,y)$ be
\begin{align}
\ellbinary((x,y), h) = \ind{yh(x) \le 0 } \perm \label{eqn:17}
\end{align}
This way, norm constrain on $w$ will be able to affect the complexity of the hypothesis class. However, the loss function is still discontinuous. We will design a surrogate continuous loss function below for analysis.  

%Assume that we are in the same setting as in the previous section. A fundamental problem we face in this setting is that we do not have a continuous loss: everything is discrete in the output space. We need to find a way to reason about the scale of the output. An example of this is logistic regression: the logistic regression model outputs a probability, and when we compare it to the outcome (0 or 1), its closeness to the true output gives us a measure of how confident we are in the prediction.

Now we formalize the notion of margin, which plays an important role in the analysis. 
As shown in Figure \ref{lec6:fig:margin}, intuitively, the black line is a ``better'' or more confident decision boundary than the red line because datapoints are further away from the black boundary than the read boundary.  
We will define the margin of a classifier to be the minimum distance from any point to the decision boundary, and thus the black line has bigger margin than the red line. 
In this section, we will prove that the larger margin and smaller norm will lead to better generalization error. 

\begin{figure}[t]
    \begin{center}
  \includegraphics[width=0.5\textwidth]{figures/margin.png}
  \end{center}
  \caption{The red and black lines are two decision boundaries. The X's are positive examples and the O's are negative examples. The black line has a larger margin than the red line, and is intuitively a better classifier.}
  \label{lec6:fig:margin}
\end{figure}

%\subsec{Formalizing margin theory} \label{sec:formal_margin}
We start with a formal mathematical definition of the margin. We will assume throughout this section that the dataset $\cD = ((x\sp{1}, y\sp{1}), \dots, (x\sp{n}, y\sp{n}))$ is \textit{separable} by the hypothesis class, that is,  exists some $h_\theta\in\cH$ that fits the data with zero error: $\forall i, y^{(i)} = \sgn(h_\theta(x^{(i)}))$. This assumption can be relaxed or weakened, but we assume it to  make the derivation cleaner. The separability assumption is empirically often true because the model family is very expressive and the training error can be close to zero. 

The first definition is the margin of a single example. 
\begin{definition}[(Unnormalized) margin]
The \textit{(unnormalized) margin} of a model $h_\theta$ for example $(x, y)$ is defined as $\margin(x) = yh_\theta(x)$. Typically, margin is only defined on correctedly labeled examples with $\sgn(h_\theta(x)) = y$. Under separability condition, all the datapoints have well-defined margin $\margin(x)\geq 0$. 
\end{definition}

Then, we can define the margin for a training dataset. 
\begin{definition}[Minimum margin] Given a dataset $S = \{(x\sp{1}, y\sp{1}), \dots, (x\sp{n}, y\sp{n})\}$, the \textit{minimum margin} of model $h_\theta$ over the dataset is defined as $\gamma_{\min} \triangleq \min_{i\in [n]} y^{(i)}h_\theta(x^{(i)})$.
\end{definition}

Our final result bound from above the generalization gap by a function of the margin and the parameter norm. In the literature, there are various bounds that involve variants of the definition of the margin. %we could derive based on what margin we use. For this current setting we are using $\gamma_{\min}$, which is the minimum margin, 
For example, some more refined generalization bounds can depend on the margin averaged over the datasets~\citep{srebro2010optimistic,wei2020improved}.

The 0-1 loss in \Cref{eqn:17}  is a discontinuous function of the $\margin(x)$. We introduce a \textit{surrogate loss}, a loss function which approximates zero-one loss but takes the scale of the margin into account. The \textbf{margin loss}, also known as \textbf{ramp loss}, is defined as 
\begin{equation}
    \ell_\gamma(t) = \begin{cases} 
      0, & t\geq \gamma \\
      1, & t\leq 0 \\
      1-t/\gamma, & 0\leq t\leq \gamma
   \end{cases} \label{lec6:eqn:ramp_loss}
\end{equation}

\begin{figure}[ht!]
    \begin{center}
  \includegraphics[width=0.5\textwidth]{figures/margin_loss.png}
  \end{center}
  \caption{Plotted margin loss.\tnotelong{also draw the binary loss; have some label for $x$ axis and $y$ axis}}
  \label{lec6:fig:marginloss}
\end{figure}
Here $\gamma$ is a tunable parameter that will be chosen in the analysis. Figure \ref{lec6:fig:marginloss} plots the ramp loss. For convenience, define $\ell_\gamma((x,y), h) \triangleq \ell_\gamma(yh(x))$. We can view $\ell_\gamma$ as a continuous version of $\err$ that is more sensitive to the scale of the margin on $[0,\gamma]$. Notice that $\err$ is always less than or equal to the $\ell_\gamma$ when $\gamma\geq 0$, i.e.
\begin{equation}
    \err((x,y), h) = \ind{yh(x) < 0}\leq \ell_\gamma(yh(x)) =\ell_\gamma ((x,y), h)
\end{equation}
holds for all $(x,y)\sim P$. Taking the expectation over $(x,y)$ on both sides of this inequality, we see that
\begin{equation}
    L(h) = \Exp_{(x,y)\sim P} \left[ \err((x,y), h) \right] \leq \Exp_{(x,y)\sim P} \left[ \ell_\gamma ((x,y), h) \right].
\end{equation}

Therefore, the population loss is bounded by the expectation of the margin loss, and so it is sufficient to bound the expectation of the margin loss in order to bound the population loss.

Define the population and empirical versions of the margin loss:
\begin{equation}
L_\gamma(h) = \Exp_{(x,y)\sim P}\l[ \ell_\gamma((x,y), h)\r], \quad \hat{L}_\gamma(h) = \sum_{i=1}^n\l [\ell_\gamma((x^{(i)},y^{(i)}), h)\r].
\end{equation}

By Corollary \ref{lec6:cor:ggap-rsbound}, we see that with probability at least $1-\delta$,
\begin{equation}
L_\gamma(h) - \hat{L}_\gamma(h)\leq 2R_S(\cF) + 3\sqrt{\frac{\log (2/\delta)}{2n}},
\end{equation}
where $\cF = \{(x,y)\mapsto \ell_\gamma((x,y), h)\mid h\in\cH\}$. Note that if we set $\gamma\leq \gamma_{\min}$, then $\hat{L}_{\gamma}(h) = 0$. This follows because by definition of $\gamma_{\min}$, $y^{(i)}h(x^{(i)})\geq \gamma_{\min}$ for any $(x^{(i)}, y^{(i)})\in \cD$. As a result, $\ell_\gamma((x^{(i)}, y^{(i)}), h) = \ell_\gamma(y^{(i)}h(x^{(i)})) = 0$ holds. Therefore, it suffices to bound $R_S(\cF)$.

We will now use \textit{Talagrand's lemma} to bound $R_S(\cF)$ in terms of $R_S(\cH)$ to remove any dependence on the loss function from the upper bound. 
 
\begin{lemma}[Talagrand's lemma] \label{lec6:lem:talagrand_lemma}
Let $\phi:\R\to\R$ be a $\kappa$-Lipschitz function. Then \begin{equation}
    R_S(\phi\circ \cH)\leq \kappa R_S(\cH),
\end{equation} 
where $\phi\circ\cH = \{z\mapsto \phi(h(z))\mid h\in\cH\}$.
\end{lemma}

We can use Talagrand's lemma directly with $\phi(t) = \ell_\gamma(t)$, which is $\frac{1}{\gamma}$-Lipschitz. We can express $\cF$ as $\cF=\ell_\gamma\circ\cH'$ where $\cH' = \{(x,y)\to yh(x)\mid h\in\cH\}$. Applying Talagrand's lemma, we see that

\begin{align}
R_S(\cF) &\leq \frac{1}{\gamma}R_S(\cH') \\
&= \frac{1}{\gamma}\Exp_{\sigma_1,\dots, \sigma_n} \l[ \sup_{h\in \cH} \frac{1}{n} \sum^n_{i=1} \sigma_i y^{(i)}h(x^{(i)}) \r] \\
&= \frac{1}{\gamma}\Exp_{\sigma_1,\dots, \sigma_n} \l[ \sup_{h\in \cH} \frac{1}{n} \sum^n_{i=1} \sigma_i h(x^{(i)})  \r] \\
&= \frac{1}{\gamma}R_S(\cH).
\end{align}

Putting this all together, we have shown that for $\gamma = \gamma_{\min}$,
\begin{align}
\Err(h) \leq L_\gamma(h) &\leq 0 + O \left( \frac{R_S(\cH)}{\gamma} \right) + \tilO \left( \sqrt{\frac{\log (2 / \delta)}{2n}} \right) \\
&= O \left( \frac{R_S(\cH)}{\min_i y\sp{i} h(x\sp{i}) } \right) + \tilO \left( \sqrt{\frac{\log (2 / \delta)}{2n}} \right).
\end{align}

In other words, for training data of the form $S = \{(x\sp{i},y\sp{i})\}_{i=1}^n \subset \mathbb{R}^d \times \{-1,1\}$, a hypothesis class~$\mathcal{H}$ and 0-1 loss, we can derive a bound of the form
\begin{equation}\label{lec7:eqn:generalization_loss}
    \text{generalization loss} \leq \frac{2R_S(\mathcal{H})}{\gamma_{\mathrm{min}}} + \text{low-order term},
\end{equation}
where $\gamma_\mathrm{min}$ is the minimum margin achievable on~$S$ over those hypotheses in $\cH$ that separate the data, and $R_S(\cH)$ is the empirical Rademacher complexity of $\cH$. Such bounds state that simpler models will generalize better beyond the training data, particularly for data that is strongly separable.

\begin{remark} \label{lec7:rmk:union_bound_margin}
Note there is a subtlety here. If we think of the dataset as random, it follows that $\gamma_{\min}$ is a random variable. Consequently, the $\gamma$ we choose to define the hypothesis class is random, which is not a valid choice when thinking about Rademacher complexity! Technically we cannot apply Talagrand's lemma with a random $\kappa$ (which we took to be $1/\gamma$). Also, when we use concentration inequalities, we implicitly assume that the $\ell_\gamma((x\sp{i}, y\sp{i}), h)$ are independent of each other. That is not the case if $\gamma$ is dependent on the data.

We sketch out how one might address this issue below. The main idea is to do another union bound over $\gamma$. Choose a family $\Gamma = \left\{ 2^k: k \in [-B, B] \right\}$ for some $B$. Then, for every fixed $\gamma \in \Gamma$, with probability greater than $1 - \delta$,
\begin{align}
\Err(h) \leq \hatL_\gamma (h) + O \left( \frac{R_S(\cH)}{\gamma} \right) + \tilO \left( \sqrt{\frac{\log \frac{1}{\delta}}{n}} \right).
\end{align}
Taking a union bound over all $\gamma \in \Gamma$, it further holds that for all $\gamma \in (0, B)$, 
\begin{align}
    \Err(h) \leq \hatL_\gamma (h) + O \left( \frac{R_S(\cH)}{\gamma} \right) + \tilO \left( \sqrt{\frac{\log \frac{1}{\delta}}{n}}\right) + \tilO \left ( \sqrt{\frac{\log B}{n}} \right ). \label{lec7:eqn:unionboundmargin}
\end{align}
Last, choose the largest $\gamma \in \Gamma$ such that $\gamma \leq \gamma_{\min}$. Then, for this value of $\gamma$, our desired bound directly follows from the bound in \eqref{lec7:eqn:unionboundmargin}. Namely, we have that $\hatL_{\gamma} (h) = 0$ and $O \left( \frac{R_S(\cH)}{\gamma} \right) = O \left( \frac{R_S(\cH)}{\gamma_{\min}} \right)$. The additional term, $\tilO\left ( \sqrt{\frac{\log B}{n} }\right )$, is the price exacted by the uniform convergence argument required to correct the heuristic bound given in \eqref{lec7:eqn:generalization_loss}.

\end{remark}
