% reset section counter
%\setcounter{section}{0}

%\metadata{lecture ID}{Your names}{date}
\metadata{7}{Spencer M.~Richards and Thomas Lew}{Feb.~3rd, 2021}

\sec{Rademacher complexity of linear models}\label{lec7:sec:lin_models}

\subsec{Linear models with weights bounded in \texorpdfstring{$\ell_2$}{L2} norm}
We begin with the Rademacher complexity of linear models using weights with bounded $\ell_2$ norm.

\begin{theorem}\label{lec7:thm:l2-thm}
    Let $\mathcal{H} = \{x \mapsto \inprod{w,x} \mid w \in \R^d, \Norm{w}_2 \le B\}$ for some constant $B > 0$. Moreover, assume $\Exp_{x \sim P}\sbr{\Norm{x}_2^2} \leq C^2$, where $P$ is some distribution and $C > 0$ is a constant. Then
    \begin{align}
        R_S(\mathcal{H}) &\le \frac{B}{n} \sqrt{\sum_{i=1}^n \Norm{x\sp{i}}_2^2},  \label{lec7:eqn:linear-sample}
        \intertext{and}
        R_n(\mathcal{H}) &\le \frac{BC}{\sqrt{n}}.  \label{lec7:eqn:linear}
    \end{align}
\end{theorem}

Generally speaking, there are two methods with which we can bound the Rademacher complexity of a model. The first method, which we used in Chapter \ref{chap:uc}, consists of discretizing the space of possible outputs from our hypothesis class, then using a union bound or covering number argument to bound the Rademacher complexity of the model. While this method is powerful and generally applicable, it yields bounds that depend on the logarithm of the cardinality of this discretized output space, which in turn depends on the number of data points~$n$. In the proof below, we will instead use a more elegant, albeit limited technique which does not rely on discretization of the output space.

\begin{proof}
We start with the proof of \eqref{lec7:eqn:linear-sample}. By definition,
\begin{align}
    R_S(\mathcal{H}) 
    &= \Exp_\sigma\sbr{ \sup_{\Norm{w}_2 \le B} \frac{1}{n} \sum_{i=1}^n\sigma_i \inprod{w,x\sp{i}} }
    \\&= \frac{1}{n} \Exp_\sigma\sbr{ \sup_{\Norm{w}_2 \le B} \inprod{w,\sum_{i=1}^n\sigma_i x\sp{i}} }
    \\&= \frac{B}{n} \Exp_\sigma\sbr{ \Norm{\sum_{i=1}^n \sigma_i  x\sp{i}}_2 }
        &&\text{($\textstyle\sup_{\Norm{w}_2 \le B} \langle w,v\rangle =B\Norm{v}_2$)}
    \\&\leq \frac{B}{n} \sqrt{ \Exp_\sigma\sbr{\Norm{ \sum_{i=1}^n \sigma_i x\sp{i} }_2^2} }
        &&\text{(Jensen's ineq. for $\alpha \mapsto \alpha^2$)} 
    \\&= \frac{B}{n} \sqrt{ \Exp_\sigma \sbr{\sum_{i=1}^n \rbr{\sigma_i^2 \Norm{x\sp{i}}_2^2 + \inprod{\sigma_ix\sp{i},\sum_{j \ne i}^n \sigma_j x\sp{j}} }} }
    \\&= \frac{B}{n} \sqrt{\sum_{i=1}^n \Norm{x\sp{i}}_2^2}.
        &&\text{($\sigma_i$ indep. and $\Exp[\sigma_i]=0$)}
\end{align}
This completes the proof of \eqref{lec7:eqn:linear-sample} for the empirical Rademacher complexity. The bound on the average Rademacher complexity in \eqref{lec7:eqn:linear} follows from taking the expectation of both sides to get
\begin{equation}
    R_n(\mathcal{H}) = \Exp\sbr{ R_S(\mathcal{H}) }
    = \frac{B}{n} \Exp\sbr{ \sqrt{\sum_{i=1}^n \Norm{x\sp{i}}_2^2} }
    \le \frac{B}{n} \sqrt{ \sum_{i=1}^n \Exp\sbr{\Norm{x\sp{i}}_2^2} }
    \le \frac{BC}{\sqrt{n}},
\end{equation}
where the first inequality is another application of Jensen's inequality, and the second follows from the assumption $\Exp_{x \sim P}\sbr{\Norm{x}_2^2} \leq C^2$.

\end{proof}

We observe that both the empirical and average Rademacher complexities scale with the upper $\ell_2$-norm bound $\Norm{w}_2 \le B$ on the parameters~$w$, which motivates regularizing the model. However, smaller weights in the model may reduce the margin $\gamma_\mathrm{min}$, which in turn hurts generalization according to \eqref{lec7:eqn:generalization_loss}.

\begin{remark}
Note that if we scale the data by some multiplicative factor, the bound on empirical Rademacher complexity $R_S(\cH)$ will scale accordingly. However, at the same time, we expect the margin to scale by the same multiplicative factor, so the bound on the generalization gap in \eqref{lec7:eqn:generalization_loss} does not change. This lines up with our intuition that the bound should not depend on the scaling of the data.
\end{remark}

\subsec{Linear models with weights bounded in \texorpdfstring{$\ell_1$}{L1} norm}
Now, we consider linear models again, except we restrict the $\ell_1$-norm of the parameters and assume an $\ell_\infty$-norm bound on the data.

\begin{theorem}\label{lec7:thm:l1-thm}
    Let $\mathcal{H} = \cbr{x \mapsto \inprod{w,x} \mid w \in \R^d, \Norm{w}_1 \le B}$ for some constant $B > 0$. Moreover, assume $\Norm{x\sp{i}}_\infty \leq C$ for some constant $C > 0$ and all points in $S = \{x\sp{i}\}_{i=1}^n \subset \R^d$. Then
    \begin{equation}
        R_S(\mathcal{H}) \leq BC\sqrt{\frac{2\log(2d)}{n}}.
    \end{equation}
\end{theorem}

To prove the theorem, we will need Massart's lemma, which provides a bound for the Rademacher complexity of a finite hypothesis class.

    \begin{lemma}[Massart's lemma]
        Suppose $\mathcal{Q} \subset \R^n$ is finite and contained in the $\ell_2$-norm ball of radius $M\sqrt{n}$ for some constant $M > 0$, i.e.,
        \begin{equation}
            \mathcal{Q} \subset \{v \in \R^n \mid \Norm{v}_2 \leq M\sqrt{n} \}.
        \end{equation}
        Then, for Rademacher variables $\sigma = (\sigma_1,\sigma_2,\dots,\sigma_n) \in \R^n$,
        \begin{equation}
            \Exp_\sigma \left[ \sup_{v\in \mathcal{Q}} \frac{1}{n}\inprod{\sigma,v} \right] \leq M\sqrt{\frac{2\log|\mathcal{Q}|}{n}}.
        \end{equation}
        As a corollary, if $\mathcal{F}$ is a set of real-valued functions satisfying
        \begin{equation}
            \sup_{f\in\mathcal{F}} \frac{1}{n}\sum_{i=1}^n f(z\sp{i})^2 \leq M^2,
        \end{equation}
        over some data $S = \{z\sp{i}\}_{i=1}^n$, then
        \begin{align}
            R_S(\mathcal{F}) \leq M\sqrt{\frac{2\log|\mathcal{F}|}{n}}, \quad\text{and}\quad
            R_n(\mathcal{F}) \leq M\sqrt{\frac{2\log|\mathcal{F}|}{n}}.
        \end{align}
    \end{lemma}

We will not prove Massart's lemma in detail. The intuition is to use concentration inequalities to bound $\frac{1}{n}\inprod{\sigma, v}$ for fixed $v$, then to use a union bound over the elements $v \in \mathcal{Q}$.

We will now prove Theorem \ref{lec7:thm:l1-thm}:

\begin{proof}[Proof of Theorem \ref{lec7:thm:l1-thm}]
    By definition,
    \begin{align}
        R_S(\mathcal{H}) &= \Exp_\sigma\sbr{ \sup_{\Norm{w}_1 \le B} \frac{1}{n} \sum_{i=1}^n\sigma_i \inprod{w,x\sp{i}} } \\
        &= \frac{1}{n} \Exp_\sigma\sbr{ \sup_{\Norm{w}_1\le B} \inprod{w,\sum_{i=1}^n\sigma_i x\sp{i}} } \\
        &= \frac{B}{n} \Exp_\sigma\sbr{ \Norm{\sum_{i=1}^n \sigma_i  x\sp{i}}_\infty  },
    \end{align}
    
    where the last equality is because $\sup_{\Norm{w}_1 \leq B}\inprod{w,v} = B\Norm{v}_\infty$, i.e., the $\ell_\infty$-norm is the dual of the $\ell_1$-norm, which is a consequence of H\"older's inequality. However, the $\ell_\infty$-norm is difficult to simplify further. Instead, we use the fact that $\sup_{\Norm{w}_1 \leq 1} \inprod{w,v}$ for any $v \in \R^d$ is always attained at one of the vertices $\mathcal{W} = \bigcup_{i=1}^d \{-e_i,e_i\}$, where $e_i \in \R^d$ is the $i$-th coordinate unit vector. Defining the restricted hypothesis class $\bar{\mathcal{H}} = \{x \mapsto \inprod{w,x} \mid w \in \mathcal{W}\} \subset \mathcal{H}$, this yields
    \begin{align}
        R_S(\mathcal{H}) &= \frac{1}{n} \Exp_\sigma\sbr{ \sup_{\Norm{w}_1 \le B} \inprod{w,\sum_{i=1}^n\sigma_i x\sp{i}} } \\
        &= \frac{B}{n} \Exp_\sigma\sbr{ \max_{w\in\mathcal{W}} \inprod{w,\sum_{i=1}^n\sigma_i x\sp{i}} } \\
        &= BR_S(\bar{\mathcal{H}}).
    \end{align}
    
    In particular, the model class $\bar{\mathcal{H}}$ is bounded and finite with cardinality $|\bar{\mathcal{H}}| = 2d$. This suggests using Massart's lemma to complete the proof. To do so, we need to confirm that $\mathcal{\bar{H}}$ is bounded with respect to the $\ell_2$-metric. Indeed, since the inner product of $x\sp{i}$ with a coordinate vector $e_j$ just selects the $j$-th coordinate of $x\sp{i}$, for any $w \in \mathcal{W}$ we have
    \begin{equation}
        \frac{1}{n}\sum_{i=1}^n \inprod{w,x\sp{i}}^2 \leq \frac{1}{n}\sum_{i=1}^n \Norm{x\sp{i}}^2_\infty \leq \frac{1}{n}\sum_{i=1}^n C^2 = C^2,
    \end{equation}
    where the last inequality uses the assumption $\Norm{x_i}_\infty \leq C$. So $\bar{\mathcal{H}}$ is bounded in the $\ell_2$-metric and finite, thus by Massart's Lemma we have
    \begin{equation}
        R_S(\mathcal{H}) = B R_S(\bar{\mathcal{H}}) \leq BC\sqrt{\frac{2\log|\bar{\mathcal{H}}|}{n}} = BC\sqrt{\frac{2\log(2d)}{n}},
    \end{equation}
    which completes the proof.
\end{proof}

\subsec{Comparing the bounds for different \texorpdfstring{$\cH$}{H}}

First, we note that for this hypothesis class of linear models, it is possible to obtain an upper bound proportional to $\sqrt{d/n}$ using the VC~dimension, which grows quickly with the data dimension~$d$. Our bound is better since it does not have as strong of a dependence on~$d$, and accounts for the norms of our model parameters and the data.

In the two subsections above, we considered two different hypothesis classes of linear models, each restricting different norms. In both cases, the bound on the average Rademacher complexity depended on the product of the norm bound on the parameters $w$ and the norm bound on each data point $x$. To determine which choice of hypothesis class is better, consider the bounds
    \begin{equation*}
        \Norm{w}_2\Norm{x}_2 \quad\text{vs.}\quad \Norm{w}_1\Norm{x}_\infty
    \end{equation*}
    and see how they compare in different settings. We consider 3 settings here:
    
    \begin{itemize}
    \item Suppose $w$ and $x$ are random variables with $w_i$ and $x_i$ close to the set of values $\{-1,1\}$. Then we have
    \begin{equation*}
        \sqrt{d}\cdot \sqrt{d} \quad\text{vs.}\quad d\cdot 1.
    \end{equation*}
    In this case, there is no difference in using either linear hypothesis class.
    
    \item If we additionally suppose $w$ is sparse with at most $k$ non-zero entries, then we have
    \begin{equation*}
        \sqrt{k}\cdot\sqrt{d} \quad\text{vs.}\quad k\cdot 1.
    \end{equation*}
    So for $d \gg k$, we have $\sqrt{kd} \gg k$ and thus $\ell_1$-norm regularization leads to a better complexity bound when $w$ is suspected to be sparse. Indeed, $\sqrt{d}\Norm{x}_\infty \approx \Norm{x}_2$ when the entries of $x$ are somewhat uniformly distributed, and so in the sparse case we have
    \begin{equation}
        \Norm{w}_2\Norm{x}_2 \geq \sqrt{d}\Norm{w}_2\Norm{x}_\infty \geq \Norm{w}_1\Norm{x}_\infty. 
    \end{equation}
    
    \item On the other hand, if $w$ is dense in the sense that $\Norm{w}_2\approx {\sqrt{d}}\Norm{w}_1$ (i.e., if all entries in $w$ are close to each other in magnitude), then
    \begin{equation}
        \Norm{w}_2\Norm{x}_2 \leq \frac{1}{\sqrt{d}}\Norm{w}_1 \cdot \sqrt{d} \Norm{x}_\infty \leq \Norm{w}_1\Norm{x}_\infty.
    \end{equation}
    In this case, it makes sense to regularize the $\ell_2$-norm instead.
    \end{itemize}
    
    In practice, other multiplicative factors enter the generalization bound, so regularizing both the $\ell_1$- and $\ell_2$-norms of the model parameters $w$ is preferable.

    Continuing with this rough style of analysis, for the hypothesis class with restricted $\ell_2$-norm, we can write the bound on the generalization gap in \eqref{lec7:eqn:generalization_loss} as
    \begin{equation}
        \text{generalization loss} \lesssim \frac{\Norm{w}_2\Norm{x}_2}{\sqrt{n}\gamma_{\mathrm{min}}} + \text{low-order term}.
    \end{equation}
    The presence of $\Norm{w}_2/\gamma_{\mathrm{min}}$ motivates both the minimum norm and the maximum margin formulations of the Support Vector Machine (SVM) problem as good methods to improve generalization performance of binary classifiers.

%*****************************************************************************
