\metadata{18}{Haoran Xu and Lewis Liu}{Nov 17th, 2021}

We venture into unsupervised learning by first studying classical (and analytically tractable) approaches to unsupervised learning. Classical unsupervised learning usually consists of specifying a latent variable model and fitting using the expectation-maximization (EM) algorithm. However, so far we do not have a comprehensive theoretical analysis for the convergence of EM algorithms because fundamentally analyzing EM algorithms involves understanding non-convex optimization. Most analysis of EM only applies to special cases (e.g., see ~\citet{xu2016global,daskalakis2016ten}) and it is not clear whether any of the results can be extended to more realistic, complex setups, without a fundamentally new technique for understanding nonconvex optimization. 
Instead, we will analyze a family of algorithms which are broadly referred to as spectral methods or tensor methods, which are a particular application of the method of moments~\citep{pearson1894} with the algorithmic technique of tensor decomposition~\citep{anandkumar2015learning}. While the spectral method appears to be not as empirically sample-efficient as EM, it has provable guarantees and arguably is more reliable than EM given the provable guarantees.

%\tnote{this paragraph require updating}
After discussing the basics of classical unsupervised learning, we will move on to modern applications of deep learning. In particular, we'll focus on theoretical interpretations of contrastive learning, which is a class of successful self-supervised learning algorithms in computer vision. 

\sec{Method of Moments for mixture models}

We begin by formally describing the unsupervised learning problem. First, assume that we are studying a family of distributions $P_{\theta}$ parameterized by $\theta \in \Theta$, where $P_{\theta}$ can be described by a latent variable model. Then, given data $x^{(i)},...,x^{(n)}$ that is sampled i.i.d. from some distribution in $\{P_\theta\}_{\theta \in \Theta}$, our goal is to recover the true $\theta$. 

Perhaps the most well-studied latent variable model in machine learning is the mixture of Gaussians. We consider the following model for the mixture of $k$ $d$-dimensional Gaussians. Let 
\begin{align}
\theta = \l ( (\mu_1, \cdots, \mu_k), (p_1, \cdots, p_k)\r ),
\end{align}
where $\mu_i\in \R^d$ is the mean of the $i$-th component and $p$ is a vector of probabilities belonging to the $k$-simplex, which represents the mixture coefficient for clusters. Formally, for $\Delta(k) \defeq \{ p: \|p\|_1 = 1, p\geq 0, p\in\R^k\}$, 
\begin{align}
    p = (p_1, \cdots, p_k) \in \Delta(k).
\end{align}
We then sample $x \sim P_\theta$ in a two-step approach: 
\begin{align}
    i &\sim \text{categorical}(p), \notag\\
    x &\sim \cN(\mu_i, I).
\end{align}
Here $i$ is called the latent variable since we only observe $x$. Here we assume the covariances of the Gaussians to be identity, but they can also be parameters that are to be learned.

There are many other latent variables that could be defined via a similar generative process, such as Hidden Markov Models, Independent Component Analysis, which we will discuss later. %, and Expectation-Maximization, but here we focus on the so-called Moment Method.

\subsec{Warm-up: mixture of two Gaussians}
We first study a simple case: the mixture of two Gaussians.
In this case, $k=2$, and we assume $p_1=p_2=\frac{1}{2}$. For simplicity, we also assume $\mu_1=-\mu_2$, that is, the means of the two Gaussians are symmetric around the origin. To simplify our notation, let $\mu_1=\mu$ and $\mu_2=-\mu$. These assumptions yield the following model for $x$:
\begin{equation}
    x \sim \frac{1}{2}\mathcal{N}(\mu,I) + \frac{1}{2}\mathcal{N}(-\mu,I).
\end{equation}
To implement the moment method, we need to complete the following two tasks:
\begin{enumerate}
    \item Estimate the moment(s) of $x$ using empirical samples.
    \item Recover parameters from the moment(s) of $x$.
\end{enumerate}

The first moment of $x$ is
\begin{align}
    M_1 &\defeq \Exp [x] \\
    &= \frac{1}{2}\Exp [x|i=1]+\frac{1}{2}\Exp[x|i=2] \\
    &= \frac{1}{2}\mu + \frac{1}{2}(-\mu) \\
    &= 0.
\end{align}
Therefore, the first moment provides no information about $\mu$. We compute the second moment as
\begin{align}
    M_2 &\defeq \Exp[xx^\top] \\
    &= \frac{1}{2}\Exp[xx^\top|i=1]+\frac{1}{2}\Exp[xx^\top|i=2]
\end{align}
To compute these expectations, consider an arbitrary $Z \sim \cN(\mu, I)$. Then,
\begin{align}
    \Exp [ZZ^\top] &= \Exp[Z] \Exp[Z]^\top + \Cov(Z) \\
    &= \mu \mu^\top + I
\end{align}
Recognizing that this second moment calculation is the same for both Gaussians in our mixture, we obtain:
\begin{align}
    M_2 &= \frac{1}{2}(\mu\mu^\top+I)+\frac{1}{2}(\mu\mu^\top+I) \\
    &=\mu\mu^\top+I
\end{align}
Since the second moment provides information about $\mu$, we can complete the two tasks required for the moment method using the second moment.

If we had access to infinite data, then we can compute the exact second moment $M_2=\mu\mu^\top+I$. Then, we can recover $\mu$ by evaluating the top eigenvector and eigenvalue of $M_2$.\footnote{This approach is known as the spectral method.} The top eigenvector and eigenvalue of $M_2$ is $\bar{\mu} \defeq \frac{\mu}{\norm{\mu}_2}$ and $\norm{\mu}_2^2+1$, respectively. 

In practice, however, we do not have infinite data. In that case, we need to estimate the second moment by an empirical average.
\begin{align}
    \widehat{M}_2=\frac{1}{n}\sum_{i=1}^nx\sp{i} {x\sp{i}}^\top
\end{align}
We can then recover $\mu$ by evaluating the top egivenvector and eigenvalue of $\widehat{M}_2$. However, we need this algorithm to be robust to errors, i.e., similar estimates, $\widehat{M}_2$, of the second moment should yield similar estimates of $\mu$. Fortunately, most algorithms we might use for obtaining the top eigenvector and eigenvalue are robust, so we can limit our attention to the infinite data case. Having outlined the moment method approach to the mixture of two Gaussians problem, we study a generalization of this problem in the sequel.

\subsec{Mixture of Gaussians with more components via tensor decomposition}

The general moment method for solving latent variable models is summarized by the following steps.
\begin{enumerate}
    \item Compute $M_1=\Exp[x]$, $M_2=\Exp[xx^\top]$, $M_3=\Exp[x\otimes x\otimes x],$ $M_4 = \cdots$. Note that $x\otimes x\otimes x$ is in $\mathbb{R}^{d\times d\times d}$ and $(x\otimes x\otimes x)_{ijk}=x_i\cdot x_j\cdot x_k$. For example, $M_{3,ijk}=\Exp[x_ix_jx_k]$.
    \item Design as algorithm $A(M_1, M_2, M_3,\dots)$ that outputs $\theta$.
    \item Show that $A$ is robust to errors in our moment estimates, i.e., we apply $A$ to $(\widehat{M}_1,\widehat{M}_2,\widehat{M}_3,...)$ in reality.
\end{enumerate}
In the sequel, we instantiate this paradigm for mixtures of $k$ Gaussians ($k\geq 3$). 

For the simplicity of demonstrating the idea, we assume $p_1 = \cdots = p_k =\frac{1}{k}$, i.e. $i \stackrel{\text{unif}} \sim[k]$, and $x\sim\mathcal{N}(\mu_i,I)$. Equivalently, 
\begin{equation}
    x\sim\frac{1}{k}\sum_{i=1}^k\mathcal{N}(\mu_i,I).
\end{equation}
In this example, we only describe steps (1) and (2) in the general algorithm described above.  

We first evaluate the first and second moments. The first moment follows from
\begin{align}
    M_1 &=\Exp[x] \\
    &=\sum_{i=1}^k\frac{1}{k}\Exp[x|i] \\
    &=\frac{1}{k}\sum_{i=1}^k\mu_i,
\end{align}
and the second moment is computed as
\begin{align}
    M_2 &= \Exp[xx^\top] \\
    &=\sum_{i=1}^k\frac{1}{k}\Exp[xx^\top|i] \\
    &=\sum_{i=1}^k\frac{1}{k}(\mu_i\mu_i^\top+I) \\
    &=\frac{1}{k}\sum_{i=1}^k\mu_i\mu_i^\top + I.
\end{align}

\subsubsection{Second moments do not suffice}
Can we recover $\mu=(\mu_1,...,\mu_k)$ from $\frac{1}{k}\sum_{i=1}^k\mu_i$ and $\frac{1}{k}\sum_{i=1}^k\mu_i\mu_i^\top$? Unfortunately, in most of the cases when $k\geq 3$, the first and second moments are not sufficent to recover $\mu$. 

One reason is the so-called ``missing rotation information'' problem. Let 
\begin{equation}
    U =\begin{bmatrix} \vrule & & \vrule \\ \mu_1 & \cdots & \mu_k \\ \vrule & & \vrule \end{bmatrix} \in\mathbb{R}^{d\times k}
\end{equation}
denote the matrix we aim to recover. Then, consider some rotation matrix $R\in\mathbb{R}^{k\times k}$. We consider $U$ versus $U R$:
\begin{align}
    \frac{1}{k}\sum_{i=1}^k\mu_i\mu_i^\top &= \frac{1}{k}U U^\top \\
    &=\frac{1}{k}(U R)(U R)^\top &\text{($RR^\top=I$)}
\end{align}
This result proves that the second moment is invariant to rotations. To prove a similar claim for the first moment, we also constrain our choice of $R$ such that
\begin{align}
    R\cdot\Vec{1}=\Vec{1}
\end{align}
Then,
\begin{align}
    \frac{1}{k}\sum_{i=1}^k\mu_i&=\frac{1}{k} U \cdot\Vec{1} \\
    &=\frac{1}{k} U R\cdot\Vec{1}
\end{align}
Therefore, the first and second moments of $U$ and $U R$ are indistinguishable, and we must consider the third moment in order to identify $U$.

\subsubsection{Computing the third moment}

The third moment is the tensor $\Exp[x \otimes x \otimes x] \in \mathbb{R}^{d \times d \times d}$. To express this expectation in terms of tractable quantities, we can condition on the Gaussian observed and average:
\begin{align}
	\Exp[x \otimes x \otimes x] = \frac{1}{k} \sum_{i=1}^k \Exp[x \otimes x \otimes x \mid i]
\end{align}

Each term in the sum now corresponds to the third moment for some multivariate Gaussian. Fortunately, Lemma~\ref{lec19:lma:gaussian_third_moment} suggests a formula for estimating its value.
\begin{lemma} \label{lec19:lma:gaussian_third_moment}
Suppose $z \in \cN(v, I)$. Then, 
\begin{align}
	\Exp[z \otimes z \otimes z] = v \otimes v \otimes v +  \sum_{l=1}^d \Exp[z] \otimes e_l \otimes e_l + \sum_{l=1}^d  e_l \otimes \Exp[z] \otimes e_l + \sum_{l=1}^d  e_l \otimes e_l \otimes \Exp[z] 
\end{align}
where $e_1,\dots,e_d$ denote the canonical basis vectors.
\end{lemma}
This lemma suggests that we can compute $v \otimes v \otimes v$ from a linear combination of $\Exp[z \otimes z \otimes z]$  and $\Exp[z]$. Also note that $\Exp[z] = v$. . 

\begin{proof}
We compute the third moment element-wise. That is,
\begin{align}
	(\Exp[z \otimes z \otimes z])_{ijk} &= \Exp[z_i  z_j z_k] \\
	&= \Exp[(v_i + \xi_i)\cdot (v_j + \xi_j) \cdot (v_k + \xi_k)]  &\text{$(z = v + \xi, \xi \sim \cN(0, I))$}\\
	&= v_i v_j v_k + \underbrace{\Exp [v_i v_j \xi_k] +  \Exp [v_i \xi_j v_k] +  \Exp [\xi_i v_j v_k]}_{=0} \nonumber \\ 
	&\quad + \Exp[v_i \xi_j \xi_k] + \Exp[v_j \xi_i \xi_k] + \Exp[v_k \xi_i \xi_j] + \Exp[\xi_i \xi_j \xi_k] \label{lec19:eqn:higher_moments} 
\end{align}
To explicitly compute the last four terms in \eqref{lec19:eqn:higher_moments}, we note that:
\begin{align}
    \Exp[\xi_i \xi_k] = \begin{cases} 0 &\text{if $i \neq k$} \\ \Exp[\xi_i^2] = 1 &\text{if $i = k$} \end{cases}
\end{align} 
and that for any choice of $i, j,$ and $k$,
\begin{align}
    \Exp[\xi_i \xi_j \xi_k] = 0.
\end{align}
Therefore, 
\begin{equation}
	(\Exp[z \otimes z \otimes z])_{ijk} = v_i v_j v_k + v_i \ind{j=k} +  v_j \ind{i=k}  +  v_k \ind{i=j} 
\end{equation}

Since $(\sum_{l=1}^d v \otimes e_l \otimes e_l)_{ijk} = \sum_{l=1}^d v_i  e_{lj}  e_{lk} = v_i \bbI(j=k)$, we have proven that:
\begin{equation}
	\Exp[z \otimes z \otimes z] = v \otimes v \otimes v +  \sum_{l=1}^d v \otimes e_l \otimes e_l + \sum_{l=1}^d  e_l \otimes v \otimes e_l + \sum_{l=1}^d  e_l \otimes e_l \otimes v.
\end{equation} 
\end{proof}

We can now apply Lemma~\ref{lec19:lma:gaussian_third_moment} to compute the third moment of the mixture of $k$ Gaussians. In particular,
\begin{align}
	\Exp[x \otimes x \otimes x] & =  \frac{1}{k} \sum_{i=1}^k \Exp[x \otimes x \otimes x \mid i] \\
	&=  \frac{1}{k} \sum_{i=1}^k \l (\mu_i \otimes \mu_i \otimes \mu_i +  \sum_{l=1}^d \mu_i \otimes e_l \otimes e_l + \sum_{l=1}^d  e_l \otimes \mu_i \otimes e_l + \sum_{l=1}^d  e_l \otimes e_l \otimes \mu_i \r ) \\
	&=  \frac{1}{k} \sum_{i=1}^k \mu_i \otimes \mu_i \otimes \mu_i +  \sum_{l=1}^d \frac{1}{k} \l (\sum_{i=1}^k \mu_i \r ) \otimes e_l \otimes e_l + \sum_{l=1}^d  e_l \otimes \frac{1}{k} \l (\sum_{i=1}^k \mu_i \r ) \otimes e_l \nonumber \\
    &\quad + \sum_{l=1}^d  e_l \otimes e_l \otimes \frac{1}{k} \l (\sum_{i=1}^k \mu_i \r) \\
	& =  \frac{1}{k} \sum_{i=1}^k \mu_i \otimes \mu_i \otimes \mu_i +  \sum_{l=1}^d \Exp[x] \otimes e_l \otimes e_l + \sum_{l=1}^d  e_l \otimes \Exp[x] \otimes e_l + \sum_{l=1}^d  e_l \otimes e_l \otimes \Exp[x]\\
\end{align} 

For notational convenience, let
\begin{equation}
    a^{\otimes 3} \defeq a \otimes a \otimes a.
\end{equation} 
So far, we have shown how to compute $\frac{1}{k} \sum_{i=1}^k \mu_i^{\otimes 3}$ from $\Exp[x^{\otimes 3}]$ and $\Exp[x]$. In the sequel, we will formalize the remaining problem, recovering $\{\mu_i\}_{i = 1}^k$ from $\frac{1}{k} \sum_{i = 1}^k \mu_i^{\otimes 3}$, as the tensor decomposition problem, and discuss efficient algorithms for it.

\paragraph{Tensor decomposition}
Recovering the Gaussian means, $\{\mu_i\}_{i = 1}^k$,  from the third mixture moment, $\frac{1}{k} \sum_{i = 1}^k \mu_i^{\otimes 3}$, is a special case of the general tensor decomposition problem. That problem is set up as follows: Assume that $a_1, \cdots a_k \in \bbR^d$ are unknown. Then, given $\sum_{i=1}^k a_i^{\otimes 3}$, our goal is to reconstruct the $a_i$ vectors. 

Before we present some standard results on tensor decomposition, we first describe some basic facts about tensors. Much like matrices, tensors have an associated rank. For example, $a \otimes b \otimes c$ is a rank-1 tensor. In general, the rank of a tensor $T$ is the minimum $k$ such that $T$ can be decomposed as 
\begin{equation}
    T = \sum_{i=1}^k a_i \otimes b_i \otimes c_i.
\end{equation} 
for some $\{a_i\}_{i =1}^k, \{b_i\}_{i =1}^k, \{c_i\}_{i =1}^k$.
Another difference between tensors and matrices is that the former objects do not have the typical rotational invariance. In particular, consider applying a right rotation $R\in \R^{k\times k}$ to the matrix \begin{equation}
A =\begin{bmatrix} \vrule & & \vrule \\ a_1 & \cdots & a_k \\ \vrule & & \vrule \end{bmatrix} \in\mathbb{R}^{d\times k}
\end{equation}
and get \al{\widetilde{A} = AR = \begin{bmatrix} \vrule & & \vrule \\ \tilde{a}_1 & \cdots & \tilde{a}_k \\ \vrule & & \vrule \end{bmatrix} \in\mathbb{R}^{d\times k}}

Then, 

\al{\sum_{i=1}^k a_ia_i^\top = AA^\top = (AR)\cdot (AR)^\top = \sum_{i=1}^k \tilde{a}_i\tilde{a}_i^\top}
%In particular, 
However, there is no tensor analogue to the rotation invariance result above. 
%\begin{equation}
%    AA^\top = A RR^\top A^\top.
%\end{equation}
But tensors do maintain an interesting (and useful) permutation invariance; that is, $T = \sum_{i = 1}^k a_i^{\otimes 3}$ is invariant to permutations of the indices of $a_i$. Or in other words, let $P\in \R^{k\times k}$ be a permutation matrix suppose, and let \al{
	\widetilde{A} = AP = \begin{bmatrix} \vrule & & \vrule \\ \tilde{a}_1 & \cdots & \tilde{a}_k \\ \vrule & & \vrule \end{bmatrix} \in\mathbb{R}^{d\times k}
}
Then, 
\al{
	\sum_{i=1}^k a_i^{\otimes 3} =  \sum_{i=1}^k \tilde{a}_i^{\otimes 3}
}
The lack of rotation invariance in the sense above and the existence of permutation invariance make tensor decomposition computationally challenging as well as powerful. 

We now summarize some standard results regarding tensor decomposition for $T = \sum_{i = 1}^k a_i^{\otimes 3}$. The results for decomposing the asymmetric version  $T = \sum_{i = 1}^k a_i\otimes b_i \otimes$ are largely analogous. We will not prove these results here.
\begin{enumerate}
\setcounter{enumi}{-1}
\tnotelong{Tengyu will add references here}
\item  In the most general case, recovering the $a_i$'s from $T$ is computationally hard. Any procedure will either fail to find a unique $a_i$ or it fails to find $a_i$ \emph{efficiently}. 
\item In the orthogonal case, i.e. $a_1,\dots,a_k$ are orthogonal vectors, each $a_i$ is a global maximizer of 
\begin{equation}
    \max_{\|x\|_2 = 1} T(x,x,x) = \sum_{i,j,k} T_{ijk} x_i x_j x_k
\end{equation}
We can heuristically think of $a_i$ as eigenvectors of $T$ and there exists an algorithm to compute $a_i$ in poly-time.
\item In the independent case, i.e. $a_1,\dots,a_k$ are linearly independent. Jennrich's algorithm can be used to efficiently recover $\{a_i\}_{i = 1}^k$.
\end{enumerate} 
Results 1 and 2 above both involve the so-called ``under-complete'' case ($k \leq d$), e.g., when the number of Gaussians in the mixture is smaller than the dimension of the data. Next, we describe certain overcomplete cases for which efficient tensor decomposition is possible.
\begin{enumerate}
\setcounter{enumi}{2}
\item Suppose $a_1^{\otimes2},\dots,a_k^{\otimes2}$ are independent for $k \leq d^2$. Then, applying Result 2, we can recover $a_i$ from $\sum_{i=1}^k (a_i^{\otimes2})^{\otimes3} = \sum_{i=1}^k (a_i^{\otimes6}) \in \bbR^{d^6}$.
\item Excluding an algebraic set of measure $0$, we can use the FOOBI algorithm to recover $a_i$ from the fourth-order tensor $\sum_{i = 1}^k a_i^{\otimes 4}$ when $k \leq d^2$. A robust version of the FOOBI algorithm is described in \citet{ma2016poly}.
\item Assume $a_i$ are \emph{randomly} generated unit vectors. Then, for the third order tensor, $k$ can be large as $d^{1.5}$ \cite{ma2016poly, schrammsteurer17}. 
\end{enumerate}

In summary, the moment method is a recipe in which we first compute high order moments (i.e. tensors), assume that these estimates are noiseless, and decompose these tensors to recover the latent variables. Though we do not discuss these results here, there is an extensive literature analyzing the robustness of the moment method to error in the moment estimates. Last, we note that even though we only explicitly analyze the mixture of Gaussians model here, latent variable models amenable to analysis by the moment method include ICA, Hidden Markov Models, topic models, etc.

\sec{Graph Decomposition and Spectral Clustering}
Introduced by \citet{shi2000normalized} and \citet{ng2001spectral}, spectral clustering learns a model for the data points using a \emph{pairwise} notion of similarity. Formally, assume that we are given $n$ data points $x\sp{1}, \dots, x\sp{n}$ as well as a similarity matrix $G \in \bbR^{n\times n}$ such that 
\begin{equation}
    G_{ij} = \rho (x\sp{i}, x\sp{j})
\end{equation}
where $\rho$ is some measure of similarity that assigns larger values to more similar pairs of points. 

For example, $x\sp{i}$ could denote images for which $\rho (x\sp{i}, x\sp{j})$ measures the semantic similarity. Alternatively, $x\sp{i}$ might be users of a social network and $\rho (x\sp{i}, x\sp{j}) = 1$ if $x\sp{i}$ and $x\sp{j}$ are friends (hence usually share similar interests / jobs / $\cdots$). 

We note that in moment methods, $\Exp[xx^\top]$ captures pairwise information between coordinates / dimensions, whereas matrix $G$ here captures pairwise information between datapoints.

Our goal is to cluster the data points by viewing $G$ as a graph. For instance, in the social network example, we can naturally view $G$ as the adjacency matrix of an \emph{unweighted} graph, where $G_{ij} \in \{0, 1\}$ defines the edges. Then, the clustering problem is to partition the graph into distinct neighborhoods, i.e., components that are as separate from each other as possible. As we will see repeatedly in the sequel, the eigendecomposition of $G$ is closely related to this graph paritioning / clustering problem.

\begin{figure}[ht]
	\centering
	\includegraphics[width=2in]{figures/ssl1.pdf}
	\caption{A demonstration of graph partitioning. \tnote{slightly more caption}}
%	\label{lec15:fig:OLgame}
\end{figure}


\subsec{Stochastic block model} 
In the stochastic block model (SBM), $G$ is assumed to be generated randomly from two hidden communities. Formally, 
\begin{equation}
    \{ 1, \cdots n \} = S \cup \bar{S},
\end{equation}
where $S$ and $\bar{S}$ partition $[n]$. Assume $|S| = \frac{n}{2}$. We then assume the following generative model for $G$. 
If $i,j \in S$ or $i,j \in \bar{S}$, then 
\begin{align}
    G_{ij} = \begin{cases}
        1 &\text{w.p. $p$} \\
        0 &\text{w.p. $1-p$} \end{cases}.
\end{align} 
Otherwise, for $i$ and $j$ in distinct components, 
\begin{align}
    G_{ij} = \begin{cases}
        1 &\text{w.p. $q$} \\
        0 &\text{w.p. $1-q$} \end{cases}
\end{align} 
for $p > q$ (i.e., more likely to be connected if from the same group). For instance, $S$ and $\bar{S}$ could mean two companies, and $i\in[n]$ is a user of a social network. Two users $i, j$ are more likely to know each other if they are in the same company.

\begin{figure}[ht]
	\centering
	\includegraphics[width=2in]{figures/ssl2.pdf}
	\caption{A graph generated by the stochastic block model with $p=\frac{2}{3}$ and $q=\frac{1}{5}$.}
	%	\label{lec15:fig:OLgame}
\end{figure}


Our goal is then to recover $S$ and $\bar{S}$ from $G$; the primary tool we use towards this goal is the eigendecomposition of $G$.

In some trivial cases, it is not necessary to eigendecompose $G$ to recover the two hidden communities. Suppose, for instance, that $p = 0.5$ and $q = 0$. Then, the graph represented by $G$ will contain two connected components that correspond to $S$ and $\bar{S}$.

As a warm-up to motivate our approach, we eigendecompose $\bar{G} = \Exp[G]$. Observe that
\begin{align}
    \bar{G}_{ij} = \begin{cases}
        p &\text{if $i,j$ from the same community} \\
        q &\text{o.w.} \end{cases}.
\end{align}
It is then easy to see that $\bar{G}$ is a matrix of rank $2$:
\begin{align}
    \bar{G} = \left[
        \begin{array}{c|c}
        p \cdots p & q \cdots q \\
        \vdots & \vdots \\
        p \cdots p & q \cdots q\\
        \hline
        q \cdots q & p \cdots p \\
        \vdots & \vdots \\
        q \cdots q & p \cdots p 
        \end{array}
        \right].
\end{align}
\begin{lemma} \label{lec19:lma:sbm_eigen}
Let $\bar{G} = \Exp[G]$ for the stochastic block model. Then, letting $u_i(A)$ denote the $i$-th eigenvector of the matrix $A$,
\begin{align}
    u_1(\bar{G}) &= \vec{1} \label{lec19:eqn:top_eig_G}\\
    u_2(\bar{G}) &= [\underbrace{1, \dots, 1}_{|S|}, \underbrace{-1, \dots, -1}_{|\bar{S}|}]^\top \label{lec19:eqn:second_eig_G}
\end{align}
where $u_2(\bar{G})$ has $|S|$ entries of $1$ and $|\bar{S}|$ entries of $-1$.
\end{lemma}

\begin{proof}
We begin by directly proving \eqref{lec19:eqn:top_eig_G}.
\begin{align}	
	\bar{G} \cdot \vec{1}  &= \begin{bmatrix}
           \frac{pn}{2} + \frac{qn}{2} \\
           \vdots \\
           \frac{pn}{2} + \frac{qn}{2}
         \end{bmatrix} \\
         &= \frac{p+q}{2} \cdot n \cdot \vec{1}.
\end{align}
More generally, $\vec{1}$ is the top eigenvector for any matrix with fixed row sum or any graph with uniform degree (i.e., regular graph). 

Next, we prove \eqref{lec19:eqn:second_eig_G}. Let 
\begin{align}
    G' = \left[
        \begin{array}{c|c}
        r \cdots r \\
        \vdots & \makebox{\text{\huge 0}} \\
        r \cdots r \\
        \hline
        & r \cdots r \\
        \makebox{\text{\huge 0}} & \vdots \\
        & r \cdots r
        \end{array}
        \right]
\end{align}
for $r = p - q$. To precisely define $G'$, we note that $G'$ is block diagonal with two blocks of size $|S|$ and $|\bar{S}|$, respectively. Then, 
\begin{align}	
	\bar{G} &= \vec{1} \vec{1}^\top q + G'. \label{lec19:eqn:barG}
\end{align}
Thus,
\begin{align}
 G' \cdot u &= \left[
    \begin{array}{c|c}
    r \cdots r \\
    \vdots & \makebox{\text{\huge 0}} \\
    r \cdots r \\
    \hline
    & r \cdots r \\
    \makebox{\text{\huge 0}} & \vdots \\
    & r \cdots r
    \end{array}
    \right] \cdot \begin{bmatrix}
           1 \\ \vdots \\ 1\\ -1 \\
           \vdots \\
           -1
         \end{bmatrix} = r \cdot \frac{n}{2} \cdot u. \label{lec19:eqn:gprimeu}
\end{align}
Then, because $u \perp \vec{1}$, we can combine \eqref{lec19:eqn:barG} and \eqref{lec19:eqn:gprimeu} to obtain
\begin{align}
\bar{G} \cdot u =  G' \cdot u =  r \cdot \frac{n}{2} \cdot u  = \frac{p-q}{2} \cdot n \cdot u
\end{align}
Thus, $u$ has eigenvalue $\frac{p-q}{2} \cdot n$. 
\end{proof}

\begin{remark}
    Lemma~\ref{lec19:lma:sbm_eigen} shows that
    \begin{equation}
        \bar{G} = \frac{p + q}{2} \cdot \vec{1} \vec{1}^\top + \frac{p - q}{2} \cdot u u^\top.
    \end{equation}
\end{remark}
More generally, when we have more than two clusters in the graph, $G'$ is block diagonal with more than two blocks. In this setting, the eigenvectors will still align with the blocks. We illustrate this below for a generic block diagonal matrix. Let 
\begin{align}
    A = \left[
        \begin{array}{c|c|c}
        1 \cdots 1 &&\\
        \vdots & \makebox{\text{\huge 0}} & \makebox{\text{\huge 0}} \\
        1 \cdots 1 &&\\
        \hline
        & 1 \cdots 1 \\
        \makebox{\text{\huge 0}} & \vdots & \makebox{\text{\huge 0}} \\
        & 1 \cdots 1 \\
        \hline
        & & 1 \cdots 1 \\
        \makebox{\text{\huge 0}}& \makebox{\text{\huge 0}} & \vdots \\
        & & 1 \cdots 1
        \end{array}
        \right]
\end{align}

Then, the three eigenvectors of $A$ are 
\begin{align}
    \begin{bmatrix}
        1 \\ \vdots \\ 1\\ 0 \\
        \vdots \\
        0 \\ 0 \\ \vdots \\ 0
      \end{bmatrix}, \begin{bmatrix}
        0 \\ \vdots \\ 0\\ 1 \\
        \vdots \\
        1 \\ 0 \\ \vdots \\ 0
      \end{bmatrix}, \begin{bmatrix}
        0 \\ \vdots \\ 0\\ 0 \\
        \vdots \\
        0 \\ 1 \\ \vdots \\ 1
      \end{bmatrix} \label{lec19:eqn:eigenmatrix}
\end{align}
Furthermore, the rows of the matrix formed by the three eigenvectors given by \eqref{lec19:eqn:eigenmatrix} clearly give the cluster IDs of the vertices in $G$. Note also that permutations of $A$ will result in equivalent permutations in the coordinates of each of the three eigenvectors.

Next, we relate this observation to the result in Lemma~\ref{lec19:lma:sbm_eigen}. While there are no negative values in the eigenvectors given in \eqref{lec19:eqn:eigenmatrix}, we observe that any linear combination of eigenvectors is also an eigenvector, so recovering a solution that look more like \eqref{lec19:eqn:second_eig_G} is straightforward. Indeed, taking linear combinations of the eigenvectors defined above shows that there is an alternative eigenbasis that includes the all-ones vector, $\vec{1}$. How for this choice of $A$, the all-ones vector is not the unique top eigenvector. For that to be the case, we require background noise in $\bar{G}$.

In reality, we only observe $G$. In the sequel, we will show that in terms of the spectrum, $G \approx \Exp[G]$. Formally, we will leverage earlier concentration results to prove that $\norm{G - \Exp[G]}_{\text{op}}$ is small. Concretely, then,
\begin{align}
    G &= (G - \Exp[G]) + \Exp[G] \\
    &= (G - \Exp[G]) + \frac{p + q}{2} \cdot \vec{1} \vec{1}^\top + \frac{p - q}{2} \cdot u u^\top
\end{align}
Rearranging, we obtain that:
\begin{align}
    G - \frac{p + q}{2} \cdot \vec{1} \vec{1}^\top &= (G - \Exp[G]) + \frac{p - q}{2} \cdot u u^\top
\end{align}
We then hope that $G - \Exp[G]$ is a small perturbation, so that the top eigenvector of $G - \frac{p + q}{2} \cdot \vec{1} \vec{1}^\top$ is close to $u$. Namely, it suffices to show that 
\begin{equation}
    \norm{G - \Exp[G]}_{\text{op}} \ll \l \|\frac{p - q}{2} \cdot uu^\top\r \|_{\text{op}}.
\end{equation}

\metadata{20}{Miria Feng and Christopher Wolff}{Dec 1st, 2021}

We will start by proving the following lemma.
\begin{lemma}
With high probability,
\begin{align}
    \norm{ G - \Exp[G] }_{\mathrm{op}} \leq O (\sqrt{n \log n} ) \;.
\end{align}
\end{lemma}

Note that this concentration inequality is different from the ones we have seen in the course so far because both $G$ and $\Exp[G]$ are matrices, not scalars. Our goal will be to turn the quantity on the LHS into something that we are familiar with.

\begin{proof}
The key idea is that we can use uniform convergence, after noting that
\begin{align}
    \norm{ G - \Exp[G] }_{\mathrm{op}} &= \max_{\norm{ v }_2 = 1} \left\vert v^\top (G - \Exp[G]) v \right\vert \\
    &= \max_{\norm{ v }_2 = 1} \left\vert v^\top G v - v^\top \Exp[G] v \right\vert \\
    &= \max_{\norm{ v }_2 = 1} \left\vert \sum_{i, j \in [n]} v_i v_j G_{ij} - \Exp \left[ \sum_{i, j \in [n]} v_i v_j G_{ij} \right] \right\vert \;.
\end{align}
Now, the quantity inside the $\max$ is the difference between the sum of independent random variables and their expectation, which we are familiar with. We can use brute force discretization to deal with the $\max$. First, note that for a fixed $v$ with $\norm{ v }_2 = 1$, we can use Hoeffding's inequality to find that
\begin{align}
    \Pr \left( \left\vert \sum_{i, j \in [n]} v_i v_j G_{ij} - \Exp \left[ \sum_{i, j \in [n]} v_i v_j G_{ij} \right] \right\vert \geq \epsilon \right) \leq \exp(-\frac{\epsilon^2}{2}) \;.
\end{align}
Then, we choose $\epsilon = O(\sqrt{n \log n})$, take a discretization of the unit ball with granularity $1/n^{O(1)}$ (which yields a cover of cardinality  $\exp(n \log n)$), and take a union bound over this discretized set to achieve the desired result.
\end{proof}

\begin{remark}
Comparing this bound to $\frac{p - q}{2} \cdot n$, we can deduce that if $p - q \gg \frac{\sqrt{\log n}}{\sqrt{n}}$, then we can recover the vector $u$ approximately. Via a post-processing step that we do not discuss here, $u$ can actually be recovered exactly. 
\end{remark}

\subsec{Clustering the worst-case graph}\label{subsec:clustering_worst_graph}

%\tnote{I wonder whether it's easier to use $A$ for adjacency matrix instead of overloading $G$}

Given a graph $G(V, E)$ where $V$ denotes the set of vertices and $E$ the set of edges, we define the {\it conductance} of a component $S$ as
\begin{align}
    \phi(S) &\defeq \frac{\vert E(S, \bar{S}) \vert}{\operatorname{Vol}(S)}
\end{align}
where $E(S, \bar{S})$ is the total number of edges between $S$ and $\bar{S}$, and $\operatorname{Vol}(S)$ is the total number of edges connecting to $S$. To be precise, let $A$ be the adjacency matrix of $G$, 
\begin{align}
    E(S, \bar{S}) &= \sum_{i \in S, j \in \bar{S}} A_{ij} \\
    \operatorname{Vol}(S) &= \sum_{i \in S, j \in [n]} A_{ij} \;.
\end{align}
Intuitively, conductance captures how separated $S$ and $\bar{S}$ are. 

Since $\operatorname{Vol}(S) \geq E(S, \bar{S})$, it follows that $\phi(S) \leq 1$. Next, observe that $\operatorname{Vol}(S) + \operatorname{Vol}(\bar{S}) = \operatorname{Vol}(V)$. Then, if $\operatorname{Vol}(S) \leq \operatorname{Vol}(V)/2$, it must be the case that $\operatorname{Vol}(S) \leq \operatorname{Vol}(\bar{S})$ and therefore $\phi(S) \geq \phi(\bar{S})$. In subsequent analysis, we assume without loss of generality that $\operatorname{Vol}(S) \leq \operatorname{Vol}(V)/2$, i.e. that $S$ is the smaller community.

Next, we define $\phi(G)$ to be the {\it sparsest cut} of $G$ if
\begin{align}
    \phi(G) &= \min_{S: \operatorname{Vol}(S) \leq \operatorname{Vol}(V)/2} \phi(S) \;.
\end{align}
We might wonder why we need to normalize by $\operatorname{Vol}(S)$ in the definition of conductance. The reason for this is that $E(S, \bar{S})$ is typically minimized when $S$ is small. Thus, without this normalization, the sparsest cut would not be very meaningful.

For instance, suppose the graph $G$ contains $n$ nodes and can be divided into two halves each containing $n/2$ nodes, and every node is connected to all the other nodes in the same half, but is connected to only $2$ nodes in the other half (as shown in Figure~\ref{fig:ssl1}). Then, we can consider two subsets $S_1$ and $S_2$, where $S_1$ contains half the nodes, and $S_2$ contains two nodes in the same half. It's easy to see that $E(S_1, \bar{S}_1) = \frac{n}{2}\cdot 2 > E(S_2, \bar{S}_2) = \frac{n}{2}$. However, the conductance of $S_1$ is $\phi(S_1) = \frac{E(S_1, \bar{S}_1)}{\operatorname{Vol}(S_1)} = \frac{\frac{n}{2}\cdot 2}{\frac{n}{2}\cdot(\frac{n}{2}-1)+\frac{n}{2}\cdot 2}\approx\frac{4}{n}$, whereas the conductance of $S_2$ is $\phi(S_2) = \frac{E(S_2, \bar{S}_2)}{\operatorname{Vol}(S_2)}=\frac{\frac{n}{2}}{n+2} \approx\frac{1}{2}$. Thus, when $n$ is large, $S_1$ is a sparser cut than $S_2$ under $\phi(\cdot)$. 


\begin{figure}[ht]
	\centering
	\includegraphics[width=2in]{figures/ssl3.pdf}
	\caption{A demonstration of the sparsest cut. $S_1$ is a sparser cut than $S_2$.}
	\label{fig:ssl1}
\end{figure}


Our goal is to find an approximate sparsest cut $\hat{S}$ such that $\phi(\hat{S}) \approx \phi(G)$.\footnote{Finding the exact sparsest cut is a computationally hard problem.} We will use eigendecomposition to approach this problem. Let $d_i = \operatorname{Vol}(\{i\})$ be the degree of node $i$, and let $D = \text{diag}(\{d_i\})$ be the diagonal matrix that contains the degrees $d_i$ as entries. Furthermore, let 
\begin{equation}
    \bar{A} = D^{-\frac{1}{2}} A D^{\frac{1}{2}}
\end{equation}
be the normalized adjacency matrix. This is equivalent to normalizing each element $G_{ij}$ of the adjacency matrix by $\frac{1}{\sqrt{d_i d_j}}$.

In most cases, it suffices to think of $G$ as a regular graph, i.e. all of its degrees are the same. Assuming $G$ is a $\kappa$-regular graph, i.e. $d_i = \kappa$; then, this normalization simply scales $A$ by $\frac{1}{\kappa}$. Let $L = I - \bar{A}$ be the Laplacian matrix. Note that any eigenvector of $L$ is also an eigenvector of $\bar{A}$. Suppose $L$ has eigenvalues $\lambda_1 \leq \hdots \leq \lambda_n$ with corresponding eigenvectors $u_1 \hdots u_n$, then $\bar{A}$ has eigenvalues $1 - \lambda_1 \geq \hdots \geq 1 - \lambda_n$ with the same eigenvectors.

\begin{theorem}[Cheeger's inequality]
The second eigenvalue of $L$, namely $\lambda_2$, is connected to the sparsest cut $\phi(G)$ as follows:
\begin{align}
    \frac{\lambda_2}{2} \leq \phi(G) \leq \sqrt{2 \lambda_2} \;.
\end{align}
Moreover, we can find $\hat{S}$ such that $\phi(\hat{S}) \leq \sqrt{2 \lambda_2} \leq 2 \sqrt{\phi(G)}$ efficiently by rounding the second eigenvector. Suppose $u_2 = [\beta_1 \cdots \beta_n]^\top \in \bbR^n$ is the second eigenvector of $L$. Then we can choose a threshold $\tau = \beta_i$ and consider $\hat{S}_i = \{ j \in [n] : \beta_j \leq \tau \}$. At least one such $\hat{S}_i$ satisfies $\phi(\hat{S}_i) \leq 2 \sqrt{\phi(G)}$.
\end{theorem}

We might wonder why this algorithm uses the second eigenvector of $\bar{A}$ in particular, and not the first eigenvector. The top eigenvector is generally not that interesting as it only captures the ``background density'' of the graph. For instance, when $A$ is $\kappa$-regular, $\vec{1}$ is the top eigenvector of $A$ and is thus also the top eigenvector of $\bar{A} = \frac{1}{\kappa} \cdot A$. More generally, we have the following lemma:


\begin{lemma}
The top eigenvector of $\bar{A}$ (respectively, the smallest eigenvector of $L$) is $u_1 = [\sqrt{d_1} \cdots \sqrt{d_n}]^\top$.	
\end{lemma}
\begin{proof}
\begin{align}
	(\bar{A} \cdot u_1)_i &= \sum_j \bar{A}_{ij} \sqrt{d}_j \\
	&= \sum_j \frac{A_{ij}}{\sqrt{d_i}\sqrt{d_j}} \sqrt{d}_j \\
	&= \frac{1}{\sqrt{d}_i} \sum_j A_{ij} \\
	&= \frac{d_i}{\sqrt{d}_i} = \sqrt{d}_i.
\end{align}
\end{proof}


To understand why the eigenvectors of the Laplacian are related to the sparsest cut, we examine the quadratic form the Laplacian. In particular, we have the following lemma:
\begin{lemma}
	Let $v\in\bbR^N$ be a vector, $L$ is the graph Laplacian. Then, 
	\begin{align}
		v^\top L v = \frac{1}{2} \sum_{(i, j) \in E} \left( \frac{v_i}{\sqrt{d_i}} - \frac{v_j}{\sqrt{d_j}} \right)^2.
	\end{align}	
\end{lemma}
\begin{proof}
\begin{align}
	v^\top L v &= v^\top I v - v^\top \bar{A} v \\
	&= \sum_{i=1}^n v_i^2 - \sum_{i, j = 1}^n v_i v_j \bar{A}_{ij} \\
	&= \sum_{i=1}^n v_i^2 - \sum_{i, j = 1}^n v_i v_j \frac{A_{ij}}{\sqrt{d_i d_j}} \\
	&= \frac{1}{2}\cdot \left(2\sum_{i=1}^n v_i^2 - 2 \sum_{(i, j) \in E} \frac{v_i}{\sqrt{d_i}} \cdot \frac{v_j}{\sqrt{d_j}}\right) \\
	&= \frac{1}{2} \sum_{(i, j) \in E} \left( \frac{v_i}{\sqrt{d_i}} - \frac{v_j}{\sqrt{d_j}} \right)^2 \;.
\end{align}
\end{proof}

If $G$ is $\kappa$-regular, then this becomes $v^\top L v = \frac{1}{2\kappa} \sum_{(i, j) \in E} (v_i - v_j)^2$. Furthermore, suppose $v \in \{0, 1\}$ is a binary vector with support $S = \operatorname{supp}(v)$. Then, 
\begin{align}
    \frac{1}{2\kappa} \sum_{(i, j) \in E} (v_i - v_j)^2 &= \frac{1}{\kappa} E(S, \bar{S}) \\
    &= \frac{1}{\kappa} E(\operatorname{supp}(v), \overline{\operatorname{supp}}(v)) \;.
\end{align}
If $|\operatorname{supp}(v)| \leq n/2$, implying $\operatorname{Vol}(S) \leq \operatorname{Vol}(V)/2$, then
\begin{align}
    \frac{v^\top L v}{\norm{v}^2_2} &= \frac{\frac{1}{\kappa} E(S, \bar{S})}{\frac{1}{\kappa} \operatorname{Vol}(S)} = \phi(S) \;.
\end{align}
The term $\frac{v^\top L v}{\norm{v}^2_2}$ is also called the {\it Rayleigh quotient}. This result nicely connects the abstract linear algebraic approach to the sparsest cut approach. Note that we only achieve an approximate sparsest cut because when we compute eigenvectors, we minimize the Rayleigh quotient \emph{without any constraints on $v$}. By contrast, the sparsest cut minimizes the Rayleigh quotient subject to the constraint that $v \in \{0,1\}^n$.

\subsec{Applying spectral clustering}

%\tnote{it's possible to merge this with the beginning of 10.2.2 if you felt that's good; please use your own assessments}
How do the ideas from the previous sections connect to our previous discussion of spectral clustering? Suppose that we have some raw data $x^{(1)} \cdots x^{(n)}$ that we'd like to group into $k$ clusters. \citet{ng2001spectral} propose to define a weighted graph $G$ such that each element 
\begin{equation}
    G_{ij} = \exp \left( -\frac{\norm{x^{(i)} - x^{(j)}}^2_2}{2\sigma^2} \right)
\end{equation}
represents a distance between two data points. Then, we compute the first $k$ eigenvectors of the Laplacian $L$ and arrange them into the columns of a matrix: 
\begin{equation}
    \begin{bmatrix} \lvert &  & \lvert \\ u_1 & \cdots &  u_k \\ \lvert &  & \lvert \end{bmatrix} \in \bbR^{n \times k}.
\end{equation} 
The $i$-th row of this matrix (which we denote by $v_i$) is then a $k$-dimensional embedding of the $i$-th example. Note that this is usually a much lower-dimensional representation than the raw data. Finally, we can use $k$-means to cluster the embeddings $\{v_1,\dots,v_n\}$.

In high dimensions, the issue with \citet{ng2001spectral}'s approach is that the training data points are all far away from each other. The Euclidean distance between points becomes meaningless, and so our definition of $G$ does not make sense. 

How do we solve this issue? \citet{haochen2021provable} propose a solution. They consider an infinite weighted graph $G(V, w)$, where $w$ are the edge weights, and $V = \cX \subseteq \bbR^n$ is the set of all possible data points. We define $w(x, x')$ to be large only if $x$ and $x'$ are very close in $\ell_2$ distance. Now, the graph is more meaningful, because only data points that are small perturbations of each other have high connection weights. However, we do not have explicit access to this graph, and its eigenvectors are infinite-dimensional. 

Now, suppose we have some eigenvector $u = (u_x)_{x \in \cX} $. Rather than explicitly represent $u_x$, we represent $u_x$ by $f_\theta(x)$ where $f_\theta$ is some parameterized model. Now, the challenge is to find $\theta$ such that $[f_\theta(x)]_{x \in \cX}$ is the second smallest eigenvector of Laplacian of $G$. It turns out solving this problem gives a form of contrastive learning, which we will discuss in Section~\ref{section:analysis_contrastive_learning}.


\sec{Self-supervised Learning}

\subsec{Pretraining / self-supervised learning / foundation model basics}
Self-supervised learning is widely used for pretraining modern models. These large pretrained models are also called foundation models~\cite{bommasani2021opportunities}. One simplified setup / workflow contains the following two stages:

\paragraph{Pretraining on unlabeled, massive data.} Let $\{x^{(1)}, \cdots, x^{(n)}\}$ be a dataset where $x^{(i)}\in\bbR^d$ is sampled from some pretraining data distribution $x^{(i)}\sim P_{\text{pre}}$. The goal is to learn a pretrained model $f_{\theta}: \bbR^d\rightarrow\bbR^k$, where $k$ is the dimension of features / representations / embeddings, and $\theta$ is the model parameter. This model can be learned by minimizing certain pretrained loss function: $\hat{L}_{\text{pre}}(\theta) = \hat{L}_{\text{pre}}(x^{(1)}, \cdots, x^{(n)}; \theta)$, which sometimes is of the form $\hat{L}_{\text{pre}}(\theta)  = \frac{1}{n}\sum_{i=1}^n \ell_{\text{pre}}(x^{(i)}; \theta)$. We use $\hat{\theta} = \argmin_{\theta}\hat{L}_{\text{pre}}(\theta)$ to denote the parameter learned during pretraining.

\paragraph{Adaptation.} During adaptation, we have access to a set of labeled downstream task examples $\{(x^{(1)}_{\text{task}}, y^{(1)}_{\text{task}}), \cdots, (x^{(n_{\text{task}})}_{\text{task}}, y^{(n_{\text{task}})}_{\text{task}})\}$, where usually $n_{\text{task}}\ll n$. One popular adapataion method is \emph{linear probe}, which uses $f_{\hat{\theta}}(x)$ as features / representations / embeddings, and train a linear classifier on downstream tasks. Concretely, the prediction on data $x$ is $w^\top f_{\hat{\theta}}(x)$, where $w$ is the linear head learned from $\min_{w} \hat{L}_{\text{task}}(w) = \frac{1}{n_{\text{task}}} \sum_{i=1}^{n_{\text{task}}} \ell_{\text{task}}(y^{(i)}_{\text{task}}, w^\top f_{\hat{\theta}}(x^{(i)}_{\text{task}}))$. Another popular adaptation method is \emph{finetuning}, which also updates the parameter $\theta$. Concretely, one initializes from $\theta = \hat{\theta}$, and solve $\min_{\theta, w} \hat{L}_{\text{task}}(w, \theta) = \frac{1}{n_{\text{task}}} \sum_{i=1}^{n_{\text{task}}} \ell_{\text{task}}(y^{(i)}_{\text{task}}, w^\top f_{{\theta}}(x^{(i)}_{\text{task}}))$.

Why does pretraining on unlabeled data with an unsupervised (self-supervised) loss help a wide range of downstream prediction tasks? There are many open questions to be answered in this field. For instance, here are three concrete questions: (1) how does pretraining help label efficiency of downstream tasks, (2) why pretraining can give ``universal'' representations, and (3) why does pretraining provide robustness to distribution shift. 

\subsec{Analysis of contrastive learning}\label{section:analysis_contrastive_learning}
Let $\bar{X}$ be the set of all natural images in the image domain, $\bar{P}_{\bar{X}}$ be the distribution over $\bar{X}$. Contrastive learning learns $f_{\theta}$ such that representations of augmentations of the same image are close, whereas augmentations of random images are far away (as visualized in Figure~\ref{fig:ssl2}). 

\begin{figure}[ht]
	\centering
	\includegraphics[width=3in]{figures/ssl5.pdf}
	\caption{A demonstration of contrastive learning. Representations of augmentations of the same image are pulled close, whereas augmentations of random images are pushed far away.}
	\label{fig:ssl2}
\end{figure}

\newcommand{\aug}[1]{\mathcal{A}(\cdot|#1)}

Given a natural image $\bar{x}\in\bar{X}$, one can generate augmentations by random cropping, flipping, adding Gaussian noise or color transformation. We use $x\sim\aug{\bar{x}}$ to denote the conditional distribution of augmentations given the natural image. For simplicity, we consider the case where Gaussian blurring is the augmentation, we have
\begin{align}
	x\sim\aug{\bar{x}} \Leftrightarrow x=\bar{x}+\xi \quad\quad\quad\quad \xi\sim\mathcal{N}(0, \sigma^2\mathcal{I}),
\end{align}
where $\norm{\xi}_2\approx\sigma\sqrt{d}$ should be $\ll\norm{\bar{x}}$. 

We say $(x, x^+)$ is a \emph{positive pair} if they are generated as follows: first sample $\bar{x}\sim \bar{P}_{\bar{X}}$, then sample $x\sim\aug{\bar{x}}$ and $x^+\sim\aug{\bar{x}}$ independently. In other words, $(x, x^+)$ are augmentations of the same natural image.

We say $(x, x')$ is a \emph{random pair / negative pair} if they are sampled as: first sample two natural images $\bar{x}\sim \bar{P}_{\bar{X}}$ and $\bar{x}'\sim \bar{P}_{\bar{X}}$, then sample augmentations $x\sim\aug{\bar{x}}$ and $x'\sim\aug{\bar{x}'}$.

The design principle for contrastive learning is to find $\theta$ such that $f_{\theta}(x)$, $f_{\theta}(x^+)$ are close, while $f_{{\theta}}(x), f_{{\theta}}(x')$ are far away~\citep{chen2020simclr, zbontar2021barlow, he2020momentum}. 

One example of contrastive learning is SimCLR~\citep{chen2020simclr}.  Given $B$ positive pairs $(x^{(1)}, x^{(1)+}), \cdots, (x^{(B)}, x^{(B)+})$, note that $(x^{(i)}, x^{(j)+})$ is a random pair if $i\ne j$, SimCLR defines the loss on the $i$-th pair as 
\begin{align}
	\text{loss}_i = -\log\frac{\exp(f_{{\theta}}(x^{(i)})^\top f_{{\theta}}(x^{(i)+}))}{\exp(f_{{\theta}}(x^{(i)})^\top f_{{\theta}}(x^{(i)+}))+ \sum_{j\ne i} \exp(f_{{\theta}}(x^{(i)})^\top f_{{\theta}}(x^{(j)+}))}.
\end{align}
Notice that $-\log\frac{A}{A+B}$ is decreasing in $A$ but increasing in $B$, the loss above encourages $f_{{\theta}}(x^{(i)})^\top f_{{\theta}}(x^{(i)+})$ to be large, while $f_{{\theta}}(x^{(i)})^\top f_{{\theta}}(x^{(j)+})$ to be small.

In the rest of this section, we consider a variant of contrastive loss: 
\begin{align}
	L(\theta)  = -2\Exp_{(x, x^+) \sim\text{positive}} f_{{\theta}}(x)^\top f_{{\theta}}(x^+) + \Exp_{(x, x') \sim\text{random}}  \left(f_{{\theta}}(x)^\top f_{{\theta}}(x')\right)^2.
\end{align}
This contrastive loss follows the same design principle as other contrastive losses in the literature: suppose all representations have the same norm, then the first term encourages the representations of a positive pair to be closer while  the second term encourages the representations of a random pair to be orthogonal to each other (hence far away). 

We can also define the empirical loss on a set of tuples $(x^{(i)}, x^{+(i)}, x'^{(i)})$ sampled i.i.d. as follows: $\bar{x}\sim\bar{P}_{\bar{X}}, x^{(i)}\sim\aug{\bar{x}^{(i)}}, x^{+(i)}\sim\aug{\bar{x}^{(i)}}$, $\bar{x}'\sim\bar{P}_{\bar{X}}$, $x'^{(i)}\sim\aug{\bar{x}'^{(i)}}$. The empirical loss is defined as 
\begin{align}
	\hat{L}(\theta) = \sum_{i=1}^n \left[-2f_{{\theta}}(x^{(i)})^\top f_{{\theta}}(x^{+(i)}) + \left(f_{{\theta}}(x^{(i)})^\top f_{{\theta}}(x'^{(i)})\right)^2\right].
\end{align}
Then the empirically learned parameter is $\hat{\theta} = \min_{\theta} \hat{L}(\theta)$. 

Suppose the downatream task is binary classification with label set $\{1, -1\}$. We define the downstream loss as 
\begin{align}
	\hat{L}_{\text{task}}(w, \theta) = \frac{1}{n_{\text{task}}} \sum_{i=1}^{n_{\text{task}}} \frac{1}{2} \left(y^{(i)}_{\text{task}} - w^\top f_{{\theta}}(x^{(i)}_{\text{task}})\right)^2.
\end{align}

We learn the linear head $\hat{w} = \argmin_w \hat{L}_{\text{task}}(w, \hat{\theta})$, and the evaluate its performance on downstream population data:
\begin{align}
	L_{\text{task}}(\hat{w}, \hat{\theta}) = \Exp\left[\frac{1}{2} \left(y_{\text{task}} - \hat{w}^\top f_{{\theta}}(x_{\text{task}})\right)^2\right].
\end{align}

We give a summary of our analysis pipeline below. 
\begin{enumerate}\setcounter{enumi}{-1}
\item{Assume expressivity, i.e., assuming $\exists \theta^*$ such that $L(\theta^*)=0$.}
\item{For large enough $n$ (e.g., $n>\text{Comp}({\mathcal{F}})/\epsilon^2$ where ${\mathcal{F}}=\{f_\theta\}$ is the function class, $\text{Comp}(\cdot)$ is some measure of complexity, $\epsilon$ is the target error), show that $\hat{L}(\theta) = L(\theta) \pm \epsilon$.}
\item{Let $\hat{\theta}$ be the parameter learned on empirical data. Since $\hat{L}(\hat{\theta})=\min_{\theta}\hat{L}(\theta) \le \hat{L}(\theta^*) \le L(\theta^*)+\epsilon$, we have
\begin{align}
	\hat{L}(\hat{\theta})\le\epsilon \Rightarrow L(\hat{\theta}) \le 2\epsilon
\end{align}
}
\item{\textbf{Key step:} (infinite data case) We prove a theorem that shows if $L(\hat{\theta})\le2\epsilon$, then there exists $w$ such that $L_{\text{task}}(\theta, w)\le \delta$, where $\delta$ is a function of $\epsilon$ and data distribution $\bar{P}$.}
\item{When we have enough downstream data $n_{\text{task}}\ge\text{poly}(k, \frac{1}{\epsilon'})$, for any $\theta$, with high probability we have (via uniform convergence) that for any $w$, 
$	\hat{L}_{\text{task}}(w, \theta) \approx L_{\text{task}}(w, \theta)\pm \epsilon'$. }
\item{Using the results in step 3 and step 4, we have $\hat{L}_{\text{task}}(\hat{w}, \hat{\theta}) = \min_{w} \hat{L}_{\text{task}}(w, \hat{\theta}) \le \min_w L_{\text{task}}(w, \hat{\theta}) + \epsilon' \le \delta + \epsilon'$. Thus, the final evaluation loss on the downstream task is $L_{\text{task}}(\hat{w}, \hat{\theta}) \le \hat{L}_{\text{task}}(\hat{w}, \hat{\theta}) +\epsilon' \le \delta + 2\epsilon'$. }
\end{enumerate}

We will now dive into the analysis of step 3 (all the other steps are from standard concentration inequalities). Recall that 
\begin{align}
		L(\theta)  = -2\Exp_{(x, x^+) } f_{{\theta}}(x)^\top f_{{\theta}}(x^+) + \Exp_{(x, x') }  \left(f_{{\theta}}(x)^\top f_{{\theta}}(x')\right)^2.
\end{align}

Our analysis requires structral assumptions on the data. In particular, we will use the graph theoretic language to describe the assumptions on population data. Let $X$ be the set of all augmented data, $P$ be the distribution of $x\sim\aug{\bar{x}}$ where $\bar{x}\sim\bar{P}_{\bar{X}}$. Let $p(x, x^+)$ be the probability density of positive pair $(x, x^+)$. We define a graph $G(V, w)$ where vertex set $V=X$ and edge weights $w(x, z) = p(x, z)$ for any $(x, z) \in X\times X$. In general, this graph may be infinitely large. To simplify math and avoid integrals, we assume $|X|=N$ where $N$ is the number of all possible augmented images (can be exponential in dimension). 

The degree of node $x$ is $p(x) = \sum_{z\in X} p(x, z)$.  We say $A\in\bbR^{N\times N}$ is the adjacency matrix of this graph if $A_{x, z} = p(x, z)$, and $\bar{A}$ is the normalized adjacency matrix if $\bar{A}_{x, z} = \frac{p(x, z)}{\sqrt{p(x)p(z)}}$. 

The following lemma shows that contrastive learning is closely related to the eigendecomposition of $\bar{A}$. 
\begin{lemma}
	Let $L(f) = -2\Exp_{(x, x^+) } f(x)^\top f(x^+) + \Exp_{(x, x') } \left(f(x)^\top f(x')\right)^2$.  Suppose $X=\{x_1, \cdots, x_N\}$, let matrix 
	\begin{equation}
	    F = \begin{bmatrix}  p(x_1)^{\frac{1}{2}} f(x_1)^\top  \\ \vdots \\  p(x_N)^{\frac{1}{2}} f(x_N)^\top \end{bmatrix}.
	\end{equation}
	Then,
	\begin{align}
		L(f) = \norm{\bar{A}-FF^\top}_F^2 +\text{const}.
	\end{align}
	Hence solving $F$ is equivalent to eigendecomposition of $\bar{A}$. 
\end{lemma}
\begin{proof}
\begin{align}
	\norm{\bar{A} - FF^\top}_F^2 &= \sum_{x, z\in X} \left(\frac{p(x, z)}{\sqrt{p(x)}\sqrt{p(z)}} - f(x)^\top f(z) \sqrt{p(x)}\sqrt{p(z)}\right)^2\\
	&= \text{const} -2 \sum_{x, z\in X} p(x, z) f(x)^\top f(z) + \sum_{x, z\in X}p(x)p(z)\left(f(x)^\top f(z)\right)^2\\
	&= \text{const} -2\Exp_{(x, x^+) \sim\text{positive}} f(x)^\top f(x^+) + \Exp_{(x, x') \sim\text{random}}  \left(f(x)^\top f(x')\right)^2.
\end{align}
\tnote{add a bit texts to explain the notation and steps}
\end{proof}

Standard matrix decomposition results tell us that the minimizer of $\norm{\bar{A}-FF^\top}_F^2 $ satisfies $F = U \cdot \text{diag}(\gamma_i^{\frac{1}{2}})$, where $\gamma_i$'s are the eigenvalues of $\bar{A}$ and $U\in\bbR^{N\times k}$ contains the top $k$ eigenvectors of $\bar{A}$ as its columns. Suppose we use $u_1, \cdots, u_N$ to represent the rows of $U$, i.e., 
	\begin{equation}
	U = \begin{bmatrix}  u_1^\top  \\ \vdots \\  u_N^\top \end{bmatrix}.
\end{equation}
Then we know $f(x_i)= p(x_i)^{-\frac{1}{2}} \cdot \text{diag}(\gamma_i^{\frac{1}{2}}) \cdot u_i$ is the minimizer of the contrastive loss. 

\newcommand{\id}[1]{\mathbbm{1}\left[#1\right]}

One interesting thing is that $\phi(x_i)$ has the same separability as $u_i$. This is because for any vector $v\in\bbR^k$, we have $\id{v^\top u_i>0} = \id{v^\top \text{diag}(\gamma_i^{-\frac{1}{2}}) f(x) >0}$, suggesting linear head $\text{diag}(\gamma_i^{-\frac{1}{2}}) v$ applied on feature $f(x_i)$ would achieve the same classification accuracy as $v$ applied on $u_i$. Thus, it suffices to analyze $u_i$'s downstream accuracy under linear head, which is exactly the spectral clustering literature has studied.

So when does spectral clustering produce good features? Recall that we've shown spectral clustering is good at graph partitioning in stochastic block model or the worst-case graph. For simplicity, let's consider a regular graph where $w(x) = \sum_{x'\in V} w(x, x') = d$. 

\begin{lemma}\label{lemma:ssl1}
	Suppose the graph $G$ can be partitioned into $2$ clusters $S_1$, $S_2$ with size $|S_1| = |S_2| = \frac{N}{2}$, such that $E(S_1, S_2)=\sum_{x\in S_1, z\in S_2} w(x, z) \le \alpha dN$. Furthermore, suppose $G$ cannot be partitioned well into $3$ clusters in the sense that for all partition $ S_1, S_2, S_3$, we have $\max\{\phi(S_1), \phi(S_2), \phi(S_3)\} \ge \rho$. 
	\begin{figure}[ht]
		\centering
		\includegraphics[width=3in]{figures/ssl4.pdf}
		\caption{A demonstration of the assumptions in Lemma~\ref{lemma:ssl1}. \tnote{a bit more caption?} 
			\tnote{I think $S_1, S_2$ are mentioned twice with different meanings in the lemma 10.10, so the figure is a bit ambiguous. Maybe replace $S_1, S_2, S_3$ by $T_1,..$ and change the figure.}
		}
	\end{figure}
	
	Then, let $g=\mathbbm{1}(S_1)\in \bbR^N$ (i.e., $g_i=1$ if $i\in S_1$), and $k\ge 6$, there exists linear classifier $b$ such that 
	\begin{align}
		\norm{Ub-g}_2^2 \lesssim \frac{\alpha}{\rho^2},
	\end{align}
where $U$ contains the top $k$ eigenvectors of $\bar{A}$ as its columns.
\end{lemma}

The above lemma essentially says that $\langle u_x, b \rangle\approx g_x$ for all data $x\in X$, where $u_x$ is the $x$-th row of $U$. 

Before proving the above lemma, we first introduce the following higher-order Cheeger inequality. 
\begin{lemma}[Proposition 1.2  in \cite{louis2014approximation}]\label{lemma:higher_order_cheeger}
	Let $G=(V, w)$ be a weight graph with $|V|=N$.  Then, there exists a partition $S_1, S_2, S_{3}$ of $V$ with 
	\begin{align*}
		\max\{\phi(S_1), \phi(S_2), \phi(S_3)\lesssim \sqrt{\lambda_{6}}.
	\end{align*}
	\tnote{could you state the contrapositive of this? [I suspect that it's easier to understand the contrapositive in this context]}
\end{lemma}

Now we give a proof of Lemma~\ref{lemma:ssl1}.
\begin{proof}[Proof of Lemma~\ref{lemma:ssl1}]
	By results in Section~\ref{subsec:clustering_worst_graph} we know that 
\begin{align}
	\frac{2}{N} g^\top L g &= \frac{1}{Nd} \sum_{x, z} (g_x - g_z)^2 w(x, z)\\
	&= \frac{1}{Nd} \left(\sum_{x\in S_1, z\in S_2} w(x, z) + \sum_{x\in S_2, z\in S_1} w(x, z)\right)\\
	&= \frac{2}{Nd} E(S_1, S_2) \\
	&\le \alpha.
\end{align}
Thus, $g$ has to be mostly in the smaller eigenspace of $L$.  Suppose $L$ has eigenvalue $0=\lambda_1\le\lambda_2 \le \cdots \le \lambda_N $, with corresponding eigenvectors $u_1, u_2, \cdots u_N$. Define matrix $U=[u_1, \cdots, u_k] \in \bbR^{N\times k}$. Suppose $\sqrt{\frac{2}{N}}g = \sum_{i=1}^N \beta_i u_i$. Since $\norm{\sqrt{\frac{2}{N}}g}=1$, we know $\sum_{i=1}^N \beta_i^2 = 1$.

Since we know $g^\top L g = \sum_{i=1}^N \beta_i^2 \lambda_i \le \alpha$, we can conclude $\sum_{i>k} \beta_i^2 \lambda_i \le \alpha$, which implies that $\sum_{i>k}\beta_i^2 \lesssim \frac{\alpha}{\lambda_{k+1}} \le \frac{\alpha}{\rho^2}$. Here we used the fact $\lambda_6 \gtrsim \rho^2$ by higher-order Cheeger inequality (Lemma~\ref{lemma:higher_order_cheeger}). Thus, we have $\norm{g-\sum_{i=1}^k \beta_i u_i}_2^2 = \norm{\sum_{i>k} \beta_i u_i}_2^2 \lesssim \frac{\alpha}{\rho^2}$ which finishes the proof. 

\end{proof}