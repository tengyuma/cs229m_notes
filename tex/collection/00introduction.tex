\setcounter{page}{5}

This monograph aims to provide an overview of modern machine learning theory, including both classical statistical learning techniques that remain highly relevant and fundamental today and newer developments in deep learning. The first six chapters on generalization bounds, along with sections on the method of moments, spectral clustering, and online learning, cover foundational concepts in statistical learning that are essential for understanding the current state of the field. The remaining chapters delve into more recent advances in deep learning theory, covering a selection of topics developed since 2013, such as non-convex optimization, implicit regularization effect and generalization theory under over-parameterization, and self-supervised learning / foundation models.

{\bf Organization.} The first chapter 

\tnote{todo}


%The overarching goal of this monograph is to offer a self-contained exposition of existing theory for modern machine learning. I attempt to select a minimal set of results and techniques from classical statistical learning theory that are still heavily relevant and necessary in the era of deep learning (the first six Chapters, Section~\ref{sec:method-of-moments} and Section~\ref{section:spectral_clustering}, and Chapter~\ref{chap:OL}), and then cover a selected subset of new topics on deep learning theory developed since 2013 (the rest of chapters). 



\noindent{\bf Notes and Acknowledgments.}


\sloppy I am deeply grateful to the many individuals who have contributed to the development of this monograph. This monograph is based on a collection of scribe notes for the courses CS229T/STATS231 and CS229M/STATS214 at Stanford University. The materials in Chapter \ref{chap:supervised}--\ref{chap:gen-bounds} are mostly based on Percy Liang's lecture notes~\citep{percynotes}, and Chapter~\ref{chap:OL} is largely based on Haipeng Luo's lectures~\cite{haipengnotes}. Kenneth Tay and John Cherian contributed significantly to the revision of these notes when they serve as the head teaching assistant for the course in winter 2020-2021 and fall 2021-2022 quarter, respectively. The original contributor to the scribe notes are Stanford students including but not limited to Anusri Pampari, Gabriel Poesia, Alexander Ke, Trenton Chang, Brad Ross, Robbie Jones, Yizhou Qian, Will Song, Daniel Do, Spencer M. Richards, Thomas Lew, David Lin, Jinhui Wang, Rafael Rafailov, Aidan Perreault, Kevin Han, Han Wu, Andrew Wang, Rohan Taori, Jonathan Lee, Rohith Kuditipudi, Kefan Dong, Roshni Sahoo, Sarah Wu, Tianyu Du, Xin Lu, Soham Sinha, Kevin Guo, Jeff Z. HaoChen, Carrie Wu, Kaidi Cao, and Ruocheng Wang.  The notes will be updated annually with new materials.  Please note that the reference list is far from complete.
